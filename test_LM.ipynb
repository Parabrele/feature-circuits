{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE : cpu\n",
      "IN_COLAB : False\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    from tqdm.notebook import tqdm, trange\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\", force_remount=True)\n",
    "    %cd /content/gdrive/MyDrive/feature-circuits\n",
    "    %pip install -r requirements.txt\n",
    "    !git submodule update --init\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    from tqdm import tqdm, trange\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "from circuit_without_tot import get_circuit, save_circuit\n",
    "from circuit_plotting import plot_circuit\n",
    "from dictionary_learning import AutoEncoder\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"DEVICE :\", DEVICE)\n",
    "\n",
    "print(\"IN_COLAB :\", IN_COLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pythia70m = UnifiedTransformer(\"EleutherAI/pythia-70m-deduped\", device=DEVICE)\n",
    "\n",
    "# pythia70m_embed = pythia70m.embed\n",
    "\n",
    "# pythia70m_resids= []\n",
    "# pythia70m_attns = []\n",
    "# pythia70m_mlps = []\n",
    "# for layer in range(len(pythia70m.blocks)):\n",
    "#     pythia70m_resids.append(pythia70m.blocks[layer])\n",
    "#     pythia70m_attns.append(pythia70m.blocks[layer].attn)\n",
    "#     pythia70m_mlps.append(pythia70m.blocks[layer].mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[3.7878, 1.5635, 2.2401,  ..., 3.4158, 1.7231, 3.2249],\n",
      "         [5.5106, 3.8430, 4.1828,  ..., 3.9052, 2.9130, 4.8242],\n",
      "         [3.2991, 1.3034, 1.5051,  ..., 2.6618, 2.3709, 1.4800],\n",
      "         [4.0855, 2.0843, 2.0335,  ..., 2.8115, 2.1017, 2.7100]]],\n",
      "       grad_fn=<AddBackward0>), (tensor([[[[ 2.6466e-01, -9.0403e-02, -1.2978e-01,  ..., -5.6741e+01,\n",
      "            5.4417e+01, -5.9252e+01],\n",
      "          [ 2.0098e+00,  2.9065e+00,  1.0229e+01,  ..., -5.6634e+01,\n",
      "            5.4601e+01, -5.9340e+01],\n",
      "          [-7.1375e+00,  6.7777e+00,  6.8730e+00,  ..., -5.7165e+01,\n",
      "            5.3907e+01, -5.9449e+01],\n",
      "          [-5.4164e+00,  4.7681e+00,  6.6199e+00,  ..., -5.6356e+01,\n",
      "            5.4439e+01, -5.9809e+01]],\n",
      "\n",
      "         [[-4.2213e-01, -1.8457e+00, -1.2065e+00,  ...,  5.4868e+01,\n",
      "           -6.0050e+01, -5.7623e+01],\n",
      "          [-5.2783e+00, -5.0359e+00, -9.8578e+00,  ...,  5.5199e+01,\n",
      "           -6.0318e+01, -5.7770e+01],\n",
      "          [ 7.1190e-01, -6.4730e+00, -9.2016e+00,  ...,  5.5326e+01,\n",
      "           -6.0386e+01, -5.7500e+01],\n",
      "          [ 7.1156e+00, -1.6251e+00, -9.7597e+00,  ...,  5.4671e+01,\n",
      "           -6.0339e+01, -5.7918e+01]],\n",
      "\n",
      "         [[-1.2041e+00,  6.6117e-01, -7.8955e-02,  ..., -5.5247e+01,\n",
      "           -5.8909e+01, -4.4609e+01],\n",
      "          [-7.5959e+00,  8.4716e-01, -9.7395e+00,  ..., -5.6355e+01,\n",
      "           -5.9296e+01, -4.2600e+01],\n",
      "          [ 3.4759e+00,  1.0792e+01, -4.8842e+00,  ..., -5.6212e+01,\n",
      "           -6.0328e+01, -4.6507e+01],\n",
      "          [ 9.8924e+00,  9.8922e+00, -6.2374e+00,  ..., -5.5868e+01,\n",
      "           -5.9604e+01, -4.4955e+01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.6419e-02, -6.1030e-02, -8.2943e-02,  ..., -5.3836e+01,\n",
      "            4.6522e+01, -4.5388e+01],\n",
      "          [-2.2128e-01, -8.8885e-01, -8.1489e+00,  ..., -5.3411e+01,\n",
      "            4.6521e+01, -4.5203e+01],\n",
      "          [ 1.2388e+00, -6.6170e+00, -6.5820e+00,  ..., -5.4410e+01,\n",
      "            4.7038e+01, -4.5495e+01],\n",
      "          [-1.7756e+00, -5.4228e+00, -7.1284e+00,  ..., -5.3696e+01,\n",
      "            4.7217e+01, -4.5648e+01]],\n",
      "\n",
      "         [[-1.0686e+00, -3.3937e-01,  1.6446e+00,  ..., -5.4598e+01,\n",
      "           -4.7368e+01, -5.1374e+01],\n",
      "          [-3.3995e+00,  1.4846e+00,  8.1627e+00,  ..., -5.4284e+01,\n",
      "           -4.7041e+01, -5.2387e+01],\n",
      "          [-2.1756e+01, -6.8210e+00,  1.1495e+01,  ..., -5.4610e+01,\n",
      "           -4.7608e+01, -5.3857e+01],\n",
      "          [ 4.8128e-01, -1.1736e+00,  1.3176e+01,  ..., -5.3634e+01,\n",
      "           -4.8344e+01, -5.1907e+01]],\n",
      "\n",
      "         [[ 3.6358e+00, -1.2234e+00,  6.6525e+00,  ...,  5.8628e+01,\n",
      "            6.1027e+01,  4.4813e+01],\n",
      "          [-3.4858e+00,  3.1910e+00,  8.2247e+00,  ...,  5.8838e+01,\n",
      "            6.1071e+01,  4.3809e+01],\n",
      "          [-1.0843e+01,  1.9885e+00,  6.1855e+00,  ...,  5.8851e+01,\n",
      "            6.1583e+01,  4.5229e+01],\n",
      "          [-5.7349e+00,  4.9838e+00,  9.9993e+00,  ...,  5.8629e+01,\n",
      "            6.0939e+01,  4.4234e+01]]]], grad_fn=<CatBackward0>), tensor([[[[ 8.3315e-02,  5.6966e-02, -1.0002e-01,  ..., -2.5011e+00,\n",
      "           -7.1591e-02,  1.9097e-01],\n",
      "          [-4.9712e-01, -1.8120e-01, -5.3007e-01,  ..., -5.1340e+00,\n",
      "            8.4176e-02,  1.7982e-01],\n",
      "          [-6.6777e-01,  2.4552e-01,  7.8747e-01,  ..., -4.2704e+00,\n",
      "            1.3026e-01,  5.1752e-01],\n",
      "          [-3.1115e-01,  8.0748e-01, -5.3770e-02,  ..., -3.4829e+00,\n",
      "           -2.8704e-02, -1.0674e+00]],\n",
      "\n",
      "         [[ 1.6812e-02, -2.5232e-02,  3.7320e-02,  ...,  1.1495e-01,\n",
      "           -2.3458e-01, -4.7100e-02],\n",
      "          [-7.7103e-02,  1.7013e-01, -1.8796e-01,  ...,  2.8429e-01,\n",
      "           -2.5615e-01, -3.9357e-01],\n",
      "          [-5.7835e-01,  4.0570e-01, -3.2941e-01,  ...,  4.9145e-01,\n",
      "           -1.2468e+00,  3.9963e-01],\n",
      "          [-1.8184e-01,  2.9869e-01, -1.3383e-01,  ...,  2.9134e-01,\n",
      "           -4.4832e-01, -7.2856e-01]],\n",
      "\n",
      "         [[-1.2894e-02,  1.0008e-01,  1.1371e-01,  ...,  2.7250e-02,\n",
      "            9.7690e-03,  5.9573e-02],\n",
      "          [ 6.0065e-01,  7.7003e-02,  9.6781e-02,  ...,  3.1631e-01,\n",
      "            4.2083e-01,  5.6562e-01],\n",
      "          [-7.0676e-01,  3.7949e-01, -5.2847e-01,  ...,  1.2100e+00,\n",
      "            1.7707e-01,  4.2101e-01],\n",
      "          [ 2.3333e-01,  2.6005e-02, -6.1562e-02,  ..., -1.1000e-02,\n",
      "            7.6599e-01, -3.3291e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8844e-03,  4.9000e-02, -7.8746e-01,  ...,  7.2892e-02,\n",
      "           -1.6016e-01,  2.6460e-01],\n",
      "          [ 3.5374e-02, -2.7786e-01, -1.0175e+00,  ..., -3.2674e-01,\n",
      "           -3.5523e-01, -4.9994e-01],\n",
      "          [ 1.0291e+00,  1.3590e-01, -1.9283e+00,  ..., -1.4780e+00,\n",
      "           -9.5001e-01,  7.5680e-01],\n",
      "          [-3.5078e-01,  3.0617e-01, -1.6267e+00,  ..., -8.3184e-01,\n",
      "            3.8006e-01,  7.1692e-01]],\n",
      "\n",
      "         [[-6.4547e-02,  1.6926e-02, -8.5745e-02,  ..., -3.7691e-02,\n",
      "           -1.0586e-01, -1.1813e+00],\n",
      "          [ 2.2205e-01, -1.7239e-01, -6.0965e-02,  ...,  4.1352e-01,\n",
      "            4.2809e-01, -6.0215e-01],\n",
      "          [-1.1843e-01, -7.9556e-01,  6.3298e-01,  ...,  7.4217e-01,\n",
      "           -2.4588e-01,  5.1964e-01],\n",
      "          [ 1.5860e-01, -9.1562e-01, -1.0752e-01,  ...,  5.9126e-01,\n",
      "            4.6661e-03, -6.6588e-01]],\n",
      "\n",
      "         [[-3.4558e-01,  1.6010e-01, -1.7282e-01,  ...,  7.7340e-03,\n",
      "           -5.7774e-01, -2.9050e-01],\n",
      "          [-8.2575e-01,  5.7391e-01,  1.5574e-01,  ..., -5.8998e-01,\n",
      "            4.1703e-02, -2.6689e-01],\n",
      "          [-4.0224e-01,  1.1015e-01, -8.9160e-01,  ...,  2.6900e-01,\n",
      "           -6.9657e-01, -5.5228e-01],\n",
      "          [-4.4202e-01, -1.8967e-01, -4.6616e-01,  ..., -4.4119e-01,\n",
      "            9.6188e-02, -3.6996e-01]]]], grad_fn=<PermuteBackward0>)))\n"
     ]
    }
   ],
   "source": [
    "pythia70m = LanguageModel(\"EleutherAI/pythia-70m-deduped\", device_map=DEVICE)\n",
    "\n",
    "with pythia70m.trace(\"I like choco\"):\n",
    "    out = pythia70m.gpt_neox.layers[-1].output.save()\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5246e-06, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pythia70m = UnifiedTransformer(\"EleutherAI/pythia-70m-deduped\", device=DEVICE, processing=False)\n",
    "\n",
    "with pythia70m.trace(\"I like choco\"):\n",
    "    out_bis = pythia70m.blocks[-1].output.save()\n",
    "\n",
    "print((out[0] - out_bis).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "pythia70m = LanguageModel(\"EleutherAI/pythia-70m-deduped\", device_map=DEVICE, dispatch=True)\n",
    "\n",
    "pythia70m_embed = pythia70m.gpt_neox.embed_in\n",
    "\n",
    "pythia70m_resids= []\n",
    "pythia70m_attns = []\n",
    "pythia70m_mlps = []\n",
    "for layer in range(len(pythia70m.gpt_neox.layers)):\n",
    "    pythia70m_resids.append(pythia70m.gpt_neox.layers[layer])\n",
    "    pythia70m_attns.append(pythia70m.gpt_neox.layers[layer].attention)\n",
    "    pythia70m_mlps.append(pythia70m.gpt_neox.layers[layer].mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if IN_COLAB:\n",
    "#     base = \"/content/gdrive/MyDrive/feature-circuits/\"\n",
    "# else:\n",
    "#     base = \"C:/Users/Grégoire/Documents/ENS/stages/AttentionGraph/Marks/feature-circuits/\"\n",
    "# path = base + \"dictionary_learning/dictionaires/pythia-70m-deduped/\"\n",
    "\n",
    "# if not os.path.exists(path):\n",
    "#     if IN_COLAB:\n",
    "#         # go to base / dictionary_learning :\n",
    "#         %cd /content/gdrive/MyDrive/feature-circuits/dictionary_learning\n",
    "#         !apt-get update\n",
    "#         !apt-get install dos2unix\n",
    "#         !dos2unix pretrained_dictionary_downloader.sh\n",
    "#         !chmod +x pretrained_dictionary_downloader.sh\n",
    "#         !./pretrained_dictionary_downloader.sh\n",
    "#         %cd /content/gdrive/MyDrive/feature-circuits\n",
    "#     else:\n",
    "#         %cd C:/Users/Grégoire/Documents/ENS/stages/AttentionGraph/Marks/feature-circuits/dictionary_learning\n",
    "#         %run ./pretrained_dictionary_downloader.sh\n",
    "#         %cd C:/Users/Grégoire/Documents/ENS/stages/AttentionGraph/Marks/feature-circuits\n",
    "\n",
    "# dictionaries = {}\n",
    "\n",
    "# d_model = 512\n",
    "# dict_size = 32768\n",
    "\n",
    "# ae = AutoEncoder(d_model, dict_size)\n",
    "# ae.load_state_dict(torch.load(path + f\"embed/ae.pt\", map_location='cpu'))\n",
    "# dictionaries[pythia70m_embed] = ae.to(DEVICE)\n",
    "\n",
    "\n",
    "# for layer in range(len(pythia70m.blocks)):\n",
    "#     ae = AutoEncoder(d_model, dict_size)\n",
    "#     ae.load_state_dict(torch.load(path + f\"resid_out_layer{layer}/ae.pt\", map_location='cpu'))\n",
    "#     dictionaries[pythia70m_resids[layer]] = ae.to(DEVICE)\n",
    "\n",
    "#     # ae = AutoEncoder(d_model, dict_size)\n",
    "#     # ae.load_state_dict(torch.load(path + f\"attn_out_layer{layer}/ae.pt\", map_location='cpu'))\n",
    "#     # dictionaries[pythia70m_attns[layer]] = ae.to(DEVICE)\n",
    "\n",
    "#     # ae = AutoEncoder(d_model, dict_size)\n",
    "#     # ae.load_state_dict(torch.load(path + f\"mlp_out_layer{layer}/ae.pt\", map_location='cpu'))\n",
    "#     # dictionaries[pythia70m_mlps[layer]] = ae.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    base = \"/content/gdrive/MyDrive/feature-circuits/\"\n",
    "else:\n",
    "    base = \"C:/Users/Grégoire/Documents/ENS/stages/AttentionGraph/Marks/feature-circuits/\"\n",
    "path = base + \"dictionary_learning/dictionaires/pythia-70m-deduped/\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    if IN_COLAB:\n",
    "        # go to base / dictionary_learning :\n",
    "        %cd /content/gdrive/MyDrive/feature-circuits/dictionary_learning\n",
    "        !apt-get update\n",
    "        !apt-get install dos2unix\n",
    "        !dos2unix pretrained_dictionary_downloader.sh\n",
    "        !chmod +x pretrained_dictionary_downloader.sh\n",
    "        !./pretrained_dictionary_downloader.sh\n",
    "        %cd /content/gdrive/MyDrive/feature-circuits\n",
    "    else:\n",
    "        %cd C:/Users/Grégoire/Documents/ENS/stages/AttentionGraph/Marks/feature-circuits/dictionary_learning\n",
    "        %run ./pretrained_dictionary_downloader.sh\n",
    "        %cd C:/Users/Grégoire/Documents/ENS/stages/AttentionGraph/Marks/feature-circuits\n",
    "\n",
    "dictionaries = {}\n",
    "\n",
    "d_model = 512\n",
    "dict_size = 32768\n",
    "\n",
    "ae = AutoEncoder(d_model, dict_size).to(DEVICE)\n",
    "ae.load_state_dict(torch.load(path + f\"embed/ae.pt\", map_location=DEVICE))\n",
    "dictionaries[pythia70m_embed] = ae\n",
    "\n",
    "\n",
    "for layer in range(len(pythia70m.gpt_neox.layers)):\n",
    "    ae = AutoEncoder(d_model, dict_size).to(DEVICE)\n",
    "    ae.load_state_dict(torch.load(path + f\"resid_out_layer{layer}/ae.pt\", map_location=DEVICE))\n",
    "    dictionaries[pythia70m_resids[layer]] = ae\n",
    "\n",
    "    # ae = AutoEncoder(d_model, dict_size).to(DEVICE)\n",
    "    # ae.load_state_dict(torch.load(path + f\"attn_out_layer{layer}/ae.pt\", map_location=DEVICE))\n",
    "    # dictionaries[pythia70m_attns[layer]] = ae\n",
    "\n",
    "    # ae = AutoEncoder(d_model, dict_size).to(DEVICE)\n",
    "    # ae.load_state_dict(torch.load(path + f\"mlp_out_layer{layer}/ae.pt\", map_location=DEVICE))\n",
    "    # dictionaries[pythia70m_mlps[layer]] = ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def metric_fn_v1(model, trg=None):\n",
    "#     \"\"\"\n",
    "#     default : return the logit\n",
    "#     \"\"\"\n",
    "#     if trg is None:\n",
    "#         raise ValueError(\"trg must be provided\")\n",
    "#     logits = model.unembed.output[:,-1,:]\n",
    "#     return logits[torch.arange(trg.numel()), trg]\n",
    "\n",
    "# def metric_fn_v2(model, trg=None):\n",
    "#     \"\"\"\n",
    "#     Return -log probability for the expected target.\n",
    "\n",
    "#     trg : torch.Tensor, contains idxs of the target tokens (between 0 and d_vocab_out)\n",
    "\n",
    "#     /!\\ here we assume that all last tokens are indeed in the last position (if padding, it must happen in front of the sequence, not after)\n",
    "#     \"\"\"\n",
    "#     if trg is None:\n",
    "#         raise ValueError(\"trg must be provided\")\n",
    "#     logits = model.unembed.output[:,-1,:]\n",
    "#     return (\n",
    "#          -1 * torch.gather(\n",
    "#              torch.nn.functional.log_softmax(model.unembed.output[:,-1,:], dim=-1),\n",
    "#              dim=-1, index=trg.view(-1, 1)\n",
    "#          ).squeeze(-1)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn_v1(model, trg=None):\n",
    "    \"\"\"\n",
    "    default : return the logit\n",
    "    \"\"\"\n",
    "    if trg is None:\n",
    "        raise ValueError(\"trg must be provided\")\n",
    "    logits = model.embed_out.output[:,-1,:]\n",
    "    return logits[torch.arange(trg.numel()), trg]\n",
    "    \n",
    "def metric_fn_v2(model, trg=None):\n",
    "    \"\"\"\n",
    "    default : return the logit\n",
    "    \"\"\"\n",
    "    if trg is None:\n",
    "        raise ValueError(\"trg must be provided\")\n",
    "    logits = model.embed_out.output[:,trg[0],:]\n",
    "    return logits[0, 0, trg[1]]\n",
    "\n",
    "def metric_fn_v3(model, trg=None):\n",
    "    \"\"\"\n",
    "    Return -log probability for the expected target.\n",
    "\n",
    "    trg : torch.Tensor, contains idxs of the target tokens (between 0 and d_vocab_out)\n",
    "\n",
    "    /!\\ here we assume that all last tokens are indeed in the last position (if padding, it must happen in front of the sequence, not after)\n",
    "    \"\"\"\n",
    "    if trg is None:\n",
    "        raise ValueError(\"trg must be provided\")\n",
    "    logits = model.embed_out.output[:,-1,:]\n",
    "    return (\n",
    "         -1 * torch.gather(\n",
    "             torch.nn.functional.log_softmax(model.embed_out.output[:,-1,:], dim=-1),\n",
    "             dim=-1, index=trg.view(-1, 1)\n",
    "         ).squeeze(-1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6393])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "clean = [\n",
    "    \"When Mary and John went to the store, John gave a drink to\"\n",
    "    for _ in range(batch_size)\n",
    "]\n",
    "patch = None\n",
    "\n",
    "trg = \" Mary\"\n",
    "trg_idx = torch.tensor([pythia70m.tokenizer.encode(trg)[0]] * batch_size, device=DEVICE)\n",
    "print(trg_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 5 : 2.504028081893921 seconds\n",
      "Now processing layer 5 with 18 features\n",
      "[428036, 428635, 430366, 432770, 433336, 435103, 437083, 444575, 445946, 449138, 449642, 451439, 452071, 454500, 457443, 458717, 458737, 458765]\n",
      "Layer 4 : 71.19254660606384 seconds\n",
      "Now processing layer 4 with 59 features\n",
      "[426292, 427617, 428036, 428635, 429814, 430366, 430713, 431013, 431236, 431270, 432770, 432875, 433336, 433538, 433553, 434039, 435004, 435103, 435248, 437083, 437582, 437956, 438170, 438445, 440085, 440208, 441947, 442157, 442265, 442819, 442905, 443826, 444575, 444843, 444940, 445946, 446073, 446824, 447449, 447889, 449138, 449642, 449663, 449831, 450998, 451296, 451439, 451503, 452071, 452850, 453938, 454108, 454500, 455574, 455720, 457443, 458717, 458737, 458765]\n",
      "Layer 3 : 192.45503115653992 seconds\n",
      "Now processing layer 3 with 81 features\n",
      "[426061, 426292, 427330, 427617, 427773, 428036, 428569, 428635, 429814, 430366, 430713, 431013, 431236, 431270, 432770, 432824, 432875, 433170, 433336, 433386, 433538, 433553, 434039, 434198, 434561, 435004, 435103, 435248, 436164, 436423, 437083, 437582, 437605, 437956, 438170, 438445, 438546, 439139, 439805, 439890, 440085, 440208, 441947, 442074, 442157, 442265, 442819, 442905, 443106, 443467, 443826, 444575, 444843, 444940, 445946, 446073, 446267, 446824, 447449, 447889, 449138, 449642, 449663, 449831, 450998, 451296, 451329, 451439, 451503, 452071, 452850, 453938, 454108, 454304, 454500, 455574, 455720, 457443, 458717, 458737, 458765]\n",
      "Layer 2 : 246.11554098129272 seconds\n",
      "Now processing layer 2 with 81 features\n",
      "[426061, 426292, 427330, 427617, 427773, 428036, 428569, 428635, 429814, 430366, 430713, 431013, 431236, 431270, 432770, 432824, 432875, 433170, 433336, 433386, 433538, 433553, 434039, 434198, 434561, 435004, 435103, 435248, 436164, 436423, 437083, 437582, 437605, 437956, 438170, 438445, 438546, 439139, 439805, 439890, 440085, 440208, 441947, 442074, 442157, 442265, 442819, 442905, 443106, 443467, 443826, 444575, 444843, 444940, 445946, 446073, 446267, 446824, 447449, 447889, 449138, 449642, 449663, 449831, 450998, 451296, 451329, 451439, 451503, 452071, 452850, 453938, 454108, 454304, 454500, 455574, 455720, 457443, 458717, 458737, 458765]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m circuit \u001b[38;5;241m=\u001b[39m \u001b[43mget_circuit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpythia70m\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpythia70m_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpythia70m_resids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdictionaries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_fn_v1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_idx\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Grégoire\\Documents\\ENS\\stages\\AttentionGraph\\Marks\\feature-circuits\\circuit_.py:270\u001b[0m, in \u001b[0;36mget_circuit\u001b[1;34m(clean, patch, model, embed, resids, dictionaries, metric_fn, metric_kwargs, aggregation, edge_threshold, steps)\u001b[0m\n\u001b[0;32m    267\u001b[0m downstream_act \u001b[38;5;241m=\u001b[39m downstream_act\u001b[38;5;241m.\u001b[39mto_tensor()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, downstream_feat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(downstream_features):\n\u001b[1;32m--> 270\u001b[0m     \u001b[43mdownstream_act\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdownstream_feat\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m     fs_grads[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m SparseAct(\n\u001b[0;32m    272\u001b[0m         act\u001b[38;5;241m=\u001b[39mupstream_act\u001b[38;5;241m.\u001b[39mact\u001b[38;5;241m.\u001b[39mgrad,\n\u001b[0;32m    273\u001b[0m         res\u001b[38;5;241m=\u001b[39mupstream_act\u001b[38;5;241m.\u001b[39mres\u001b[38;5;241m.\u001b[39mgrad\n\u001b[0;32m    274\u001b[0m     )\n\u001b[0;32m    275\u001b[0m     upstream_act\u001b[38;5;241m.\u001b[39mact\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mzeros_like(upstream_act\u001b[38;5;241m.\u001b[39mact)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "circuit = get_circuit(\n",
    "    clean, patch,\n",
    "    pythia70m,\n",
    "    pythia70m_embed, pythia70m_resids,\n",
    "    dictionaries,\n",
    "    metric_fn_v1, {\"trg\": trg_idx},\n",
    "    edge_threshold=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cpu :\n",
    "    - 1 : 2m47\n",
    "    - 2 : /\n",
    "    - 10: Stop at 68m+\n",
    "\n",
    "- gpu :\n",
    "    - 1 : 42s\n",
    "    - 2 : 1m32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submod_1 = \"resid_0\"\n",
    "submod_2 = \"resid_1\"\n",
    "\n",
    "weights = circuit[1][submod_1][submod_2]\n",
    "weights = weights.values()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "alive_downstream = circuit[1][submod_1][submod_2].indices()[0]\n",
    "set_downstream = list(set([alive_downstream_.item() for alive_downstream_ in alive_downstream]))\n",
    "\n",
    "ss = []\n",
    "abss = []\n",
    "nb_k = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for k in tqdm(set_downstream):\n",
    "    weights = []\n",
    "    for i, idx in enumerate(alive_downstream):\n",
    "        if idx == k:\n",
    "            weights.append(circuit[1][submod_1][submod_2].values()[i])\n",
    "    weights = torch.stack(weights)\n",
    "\n",
    "    perm = torch.argsort(weights.abs(), descending=True)\n",
    "    weights = weights[perm]\n",
    "    tot = sum(weights)\n",
    "    s = 0\n",
    "    for i in range(len(weights)):\n",
    "        s += weights[i]\n",
    "        if i < len(ss):\n",
    "            ss[i] += (s / tot).item()\n",
    "        else:\n",
    "            ss.append((s / tot).item())\n",
    "        if i < len(abss):\n",
    "            abss[i] += weights[i].abs().item()\n",
    "        else:\n",
    "            abss.append(weights[i].abs().item())\n",
    "        if i < len(nb_k):\n",
    "            nb_k[i] += 1\n",
    "        else:\n",
    "            nb_k.append(1)\n",
    "        # print(\"i :\", i)\n",
    "        # print(\"weight :\", embed_weights[i].item())\n",
    "        # print(\"% of total :\", s.item() / tot.item() * 100)\n",
    "\n",
    "ss = [ss[i] / nb_k[i] for i in range(len(ss))]\n",
    "abss = [abss[i] / nb_k[i] for i in range(len(abss))]\n",
    "\n",
    "\"\"\"\n",
    "plot ss and abss on two different axis with the same x-axis on the same plot\n",
    "\"\"\"\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('weight index')\n",
    "ax1.set_ylabel('cumulative % of total', color=color)\n",
    "ax1.plot(ss, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('weight', color=color)\n",
    "ax2.plot(abss, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_weights = 100\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('weight index')\n",
    "ax1.set_ylabel('cumulative % of total', color=color)\n",
    "ax1.plot(ss[:max_weights], color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('weight', color=color)\n",
    "ax2.plot(abss[:max_weights], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import circuit_plotting\n",
    "importlib.reload(circuit_plotting)\n",
    "circuit_plotting.plot_circuit(circuit[0], circuit[1], save_dir='./circuit/cpu_2_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = []\n",
    "for key, value in circuit[1].items():\n",
    "    for k, v in value.items():\n",
    "        all_weights.append(v.values())\n",
    "        \n",
    "all_weights = torch.cat(all_weights, dim=0)\n",
    "print(all_weights.shape)\n",
    "print(all_weights.abs().mean())\n",
    "\n",
    "plt.hist(all_weights[all_weights.abs() > 0.01].detach().cpu().numpy(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(1, 10, 50)\n",
    "B = torch.randn(1, 10, 50)\n",
    "\n",
    "print((A * B).shape)\n",
    "print(A @ B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dummy_2d_sparse_idx = torch.tensor([[0, 99, 27], [1, 2, 199]])\n",
    "dummy_2d_sparse_values = torch.randn(2, 3)\n",
    "\n",
    "dummy_2d_sparse = torch.sparse_coo_tensor(\n",
    "    dummy_2d_sparse_idx,\n",
    "    dummy_2d_sparse_values,\n",
    "    size=(100, 200)\n",
    ")\n",
    "\n",
    "print(dummy_2d_sparse.to_dense())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
