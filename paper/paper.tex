\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{natbib}

\widowpenalty=10000
\clubpenalty=10000

\begin{document}

% DELETE flags indicate parts of the text that can be deleted if there is not enough space.

\title{Title}
\author{author}
\date{date}
\maketitle

% Make the abstract

\newpage
\begin{abstract}
TODO : write the abstract
\end{abstract}

\newpage
\tableofcontents

\newpage
\section{Introduction}

%%%%%%%%%%
% NEXT TODO (not up to date) :
%   - define ROIs as functionally similar features in a single layer (residual or attn/mlp layer (no more attn head level)) and build a graph on these ROIs.
%   - on n small tasks, compare overlap of nodes between these tasks, do it per layer. 
%   - I then essentially have a "ground truth" separation of nodes, is the SBM able to find it ? Even if we restrict ourselves to maybe later layers ?
%   - USE TOY MODELS !
%   - the vision paper used corr on a single layer. Do the same. Maybe this is better. It allows for trivial corr in any feature space.
%   (future) - define structural connectivity an run experiments for it
%   - run functional connectivity experiments on more examples (let it run a day or 2) residual and not residual
%       - as brains have to be energy efficient, it is safe to assume that activation and attribution are the same.
%       - for NN, it is much less obvious as some subspaces which can be specific to some task that is not currently present might still be active just because it can, and learning to not activate might be harder than just activating for nothing... this is all jibberish...
%   - run effective connectivity
%   - do it both for id, SVD and head-level (~ROI)
%   - do it all for wikipedia, task specific, and across tasks

%   - for any graph G given by some method above :
%       - SBM, nested SBM, Louvain, spectral
%       - visualise them all
%       - do they correspond (across methods and across graphs) ?
%       - faithfulness ? task specific faithfulness/completeness/just activation ?
%       - interpret blocks
%
% (future) do it across model sizes and across training steps, to see the emergence of these structures, and if there are similarities between model sizes.
%%%%%%%%%%

%%%%%%%%%%
%%%%%
% SBM Plots & experiments :
%%%%%

% TODO : compare SBMs and community detections algorithms and show that they discover more functionally relevant structures in neural networks.

% Restart Louvain, spectral and SBM, plot hist of modularity, color by number of communities.

% fig 5.9 : do the same : correspondance between SBM blocks and louvain communities

% fig 3.6 5.10 : plot collapsed blocks graph and matrix of probabilities
%%%%%%%%%%

The field of mechanistic interpretability (MI) aims at explaining the inner workings of a neural network. This is usually done either by attributing some function or behavior to a specific computational entity \citep{TODO:probing,vision_clusters,layerjesaisplusquoi-alexandre}, or by attributing it to a subgraph of the computational graph \citep{TODO:circuit_discovery}. We see in section \ref{sec:parallels} that both of these approaches have their equivalent in the field of neurosciences.

Recent work in MI aims at finding computational subgraphs responsible for some selected task or behavior \citep(TODO), often simply trying out new ideas with lack of common frameworks or definitions to clearly state the fundamental differences between all these studies \small{\textit{they don't even mention it, they just say "oh look at our new method, in our circuits we find THREE heads in common with this other method ! We are glad to not provide any comment on the superficial let alone fundamental differences between these methods and why the heck we even tried a new one !}}. We believe that settling on unified definitions and frameworks can allow researchers to have a clearer global view of the field, of the work that has already been done and the directions we might want to explore.

We are interested in defining computational graphs of neural networks as the study of their structure could lead to new insight in the inner working of these models. It can lead to new ways of thinking about the alignment problem through interventions on these graphs. Studying the similarities between the inner working of different models, across parameter sizes or across training steps, could also lead to new insights on the evolution and generality of behaviors in these models, although this is left for future work.

This work is organised as follows. In section \ref{sec:meta}, we give general comments on the internship. In section \ref{sec:background}, we give some background on notions from neurosciences (section \ref{sec:brain_networks}), network sciences (section \ref{sec:SBM}) and mechanistic interpretability (section \ref{sec:attribution}, \ref{sec:SAE}, \ref{sec:circuit_discovery}). These notions are the basics upon which we build our work and contributions. In section \ref{sec:parallel}, we draw parallels between the way neurosciences and mechanistic interpretability explain complex systems through the analysis of underlying networks or structures. Following sections contain our contributions.

\paragraph{contribution} We use concepts from both neurosciences and MI to propose new ways of automating interpretability in neural networks.
% TODO : remove itemize when all done.
\begin{itemize}
    \item[Mostly Done] We define new low granularity nodes for neural networks study as groups of functionally linked fine grained nodes\footnote{Fine graine nodes can be anything from canonical dimentions - also known as neurons - to sparse features through any change of basis fitted to the model internal geomerty.} (section \ref{sec:TODO}) as defined by Stochastic Blockmodels (section \ref{sec:SBM}). We call these regions of interest (ROIs).
    \item[TODO] We establish the relevance of these ROIs through interventions and predictable behavioral changes in the model (section \ref{sec:TODO}).
    \item[TODO] We show that these ROIs are more functionally relevant than attention heads (section \ref{sec:TODO}), often used in the literature as low granularity nodes, as well as what would be given by community detection algorithms commonly used in the literature (see for example \citet{lu2019checking, filan2021clusterability, bushnaq2024using}) in section \ref{sec:TODO}.
    \item[Done] We define a novel algorithm to get computational graphs in neural networks that can incorporate several levels of abstraction (section \ref{sec:TODO}) along ways to study such graphs (section \ref{sec:TODO}).
    \item[TODO] We establish the superiority of Stochastic Blockmodels over more traditional community detection algorithms in studying the structure of these graphs \ref{sec:TODO}.
    \item[Done but can do better] We show the relevance of these methods by demonstrating the functional equivalence between the original model and the one induced by our computational graph (section \ref{sec:TODO}), as well as intervening on this graph with predictable effects on the resulting model's behavior (section \ref{sec:TODO}).
    \item[TODO future work] We show that restraining a model to it's elicited computational graph can naturally remove sleeper agents without intervention and that interventions on this graph can further remove unwanted behaviors (section \ref{sec:TODO}).
\end{itemize}

% Since we need a fine grained graph we will chose to build it from sparse dictionaries in a similar way to Marks although with some differences. We propose very general algorithm to build computational graphs.

% For the internship report, 1 page for the abstract, one page for the table of contents, 
% For the internship report, make a section explaining how the internship went.
\section{Meta Information}
\label{sec:meta}

TODO : fill this section

This section contains meta informations about my internship.

\subsection{Supervision and environment}

supervisor very present, discussions on a daily basis, very helpful and supportive. Organised many meetings with other teams to allow me to present my work, get feedback and find potentially relevant collaborations

team very welcoming and open to discussions

\subsection{Events}

hikes, seminars, meetings both with the team and with other teams to present our works, find new ideas and potential collaborations.

HAAISS summer school, ICML conference

\subsection{Timeline}

1st month : in depth discovery of the field of MI, exploration of several potentially interesting directions

2nd month : focus on one direction : use attribution methods on feature edges to get the graph of dependencies

3rd month : nothing works :c (well, some things work but I'm not satisfied)

4th month : Not finished. Discovery of the SBM literature, new directions. %Meetings with neuroscientists ?

5th month : Soon. (maybe too soon...)

\section{Background}

This section gives some background on notions from neurosciences (section \ref{sec:brain_networks}), network sciences (section \ref{sec:SBM}) and mechanistic interpretability (section \ref{sec:attribution}, \ref{sec:SAE}, \ref{sec:circuit_discovery}). These notions are the fundations upon which we build our work.

\subsection{Notations}

We will denote by $\mathcal{M}$ a transformer model, and by $m$ its \textit{modules} - transformer blocks, MLP layers, attention layers or attention heads. The dimension of it's residual stream will be denoted by $d_{model}$. When $\mathcal{M}$ denotes a model with an arbitrary architecture, $d_{model}$ will be the dimension of the input.

The output of module $m$ when given input $x$ will be denoted by $x^m$.

Scalar functions depending on the evaluation of $x$ by $\mathcal{M}$ will be denoted by $\lambda(\mathcal{M}, x)$. It can be a metric function on the output of the model, or any scalar function depending on its internal activations.

In section \ref{sec:SAE}, we introduce $d_{dict}$ the size of a dictionary, as well as $b_i$ and $\alpha_i$, respectively the feature vectors of a dictionary and their activation~-~or magnitude~-~for some input. We will often identify these two concepts under the notation $f_i$ or simply $f$, for feature.

\subsection{Brain Networks}
\label{sec:brain_networks}

For decades now, the field of neurosciences has been trying to understand the brain by explaining the function of each of its parts [TODO : ref]. There are two main paradigm when trying to explain the function of the brain : \textit{functional segregation} and \textit{functional integration} of brain regions [TODO : ref]. The former, which historically appeared first, assumes that the brain is divided into many parts, each specialising in various functions by engaging in local processes, while the latter assumes that the brain is a complex network of neuronal elements whose connections facilitate different functional processes. \textit{Functional integration} has been shown to be more successful in explaining higher cognitive processes like visual recognition, social cognition, emotions or others [TODO : ref].%(see, e.g., Van Den Heuvel and Sporns, 2013, for a review)

When it comes to building a brain network, three major classes of connectivity can be distinguished : \textit{structural}, \textit{functional} and \textit{effective} connectivity [TODO : ref]. Structural connectivity refers to the anatomical connections between brain regions. Functional connectivity measures temporal correlations between the activation of brain regions, the idea being that regions that share the same function, or work together in the emergence a more complex ones should be active at the same time. Effective connectivity measures causal relationships between brain regions through actual information flow.

\subsection{Block Models}
\label{sec:SBM}

TODO : this section is too ugly, there are just a few big blocks of text ! Make it more readable ! Maybe illustrations will help.

% TODO : maybe pavlovic has some good references for SBM other than peixoto

Block Models [TODO : ref] are a class of generative models for graphs. They assume that the nodes of a graph are divided into $B$ blocks, and that the probability $P_{b_i, b_j}$ of an edge between two nodes $i$ and $j$ depends only on the blocks they belong to - $b_i$ and $b_j$ respectively. The adjacency matrix $A$ is then generated by sampling each entry $A_{ij}$ from a Bernoulli distribution with parameter $P_{b_i, b_j}$.

This is a generalisation in blocks of the Erdos-Renyi model [TODO : ref], which corresponds to the case where $B = 1$. It can model a large variety of graph structures, some of which are presented in figure \ref{fig:SBM_structures}.

One can either use this framework to generate random graphs with a given structure, or to infer the underlying structure of a given graph. In the latter case, the goal is to find the block structure that maximises the likelihood of the observed graph. We will focus on the latter and fit Stochastic Block Models (SBMs) [TODO : ref] to a given graph - in our case, it will be the computational graph of a neural network.

Community detection algorithms are commonly used in the fields of neurosciences [TODO : ref] and mechanistic interpretability [TODO : ref] to try to find the underlying structure of computational graphs and hopefully explain some mechanisms of the overall function of the model. However, these algorithms assume that nodes are arranged in communities - where nodes are densely connected within communities, but sparsely between communities - which restricts to possibilities on the discovered structures. Moreover, they are often prone to overfitting, where community structure is discovered even though it is merely an artefact of random fluctuations [TODO : ref] - see figure \ref{fig:CommunityOvFitting}.

SBMs allows to relax these assumptions and find more general structures. Through the choice of clever prior distributions in bayesian models [TODO : ref], they allow to avoid the pitfall of underfitting as well as overfitting statistical fluctuations in the data by finding blocks that do not account for statistically significant structure in the data that can't be explained by random fluctuations. Underfitting is mainly avoided through recursive block models to get the likelihood of some choice for the probabilities between blocks, which also enables the discovery of nested structures in the data. Efficient inference algorithms arise from the choice of these priors and bayesian framework, using Markov Chain Monte Carlo methods to find the block structure that maximises the likelihood of the observed graph.

SBMs were succesfully used in neurosciences [TODO : ref] to identify established brain structures and advance the field as a whole. They were able to identify both functional segregation in modular parts of the network and functional integration in densely connected cores.

\subsection{Attribution and explanations}
\label{sec:attribution}

In this section, we discuss classical attribution methods in neural networks. Attribution is the process of explaining a model's prediction by attributing it to some part of the input or intermediate feature. This is a very active field of research, and many methods have been proposed.

We begin with the most basic and popular methods. \citet{springenberg2015strivingGuidedBackprop} used gradient in vision models to highligh which pixels of an image best explain some prediction. Let $\lambda : x \in \R^{d_{model}} \mapsto \lambda(x) \in \R$ be a scalar function on the model's computations - say the output value for a given class in a classifier - and $x \in \R^{d_{model}}$ an input image of size $d_{model}$.

$$ Attr(x_i) = \frac{\partial \lambda(x)}{\partial x_i} $$

They show that even though results are satisfactory, the explanation is really noisy and imprecise. They introduce guided backprop, a variant that removes negative gradients in $ReLU$ layers in the backward gradient computation, which seems to improve the quality of the explanation.
% DELETE
Thus, the attribution when passing through a $ReLU$ layer is not
$$ Attr^l = (x^l > 0) \cdot Attr^{l+1} $$
which would be the gradient, but
$$ Attr^l = (x^l > 0) \cdot (Attr^{l+1} > 0) \cdot Attr^{l+1} $$
which is the masked gradient, kept only when positive. $x^l$ and $Attr^l$ are respectively the activation and attribution of the layer $l$. For non $ReLU$ layers, the attribution is simply the gradient.

Later on, \citet{zhou2016CAM} and \citet{selvaraju2017GradCAM} introduced CAM, GradCAM and guided GradCAM, methods that build upon these results to refine the attribution of the model's prediction to the input specifically for convolutional layers. Another noteworthy method for attribution is Layer-Wise Relevance Propagation \citep{montavon2019layerLRP}. Methods purely based on gradients most notably suffer from the fact that the model is not a linear function and that we don't know which directions pointed by the gradient are relevant - unless there is some sense of counterfactual input, as in \citep{syed2023attributionAtP}.

Building upon this work, \citet{sundararajan2017axiomaticIG} introduced Integrated Gradients along with a set of axioms desirable for an attribution method. This method can be formalised in a general manner as follows. Let $\lambda : x \in \R^{d_{model}} \mapsto \lambda(x) \in \R$ be a scalar function, $x_{clean} \in \R^{d_{model}}$ an input, $x_{baseline} \in \R^{d_{model}}$ a baseline input. Let also $\gamma : [0, 1] \rightarrow \R^{d_{model}}$ be a path function between $x_{baseline}$ and $x_{clean}$. They define the attribution function as follows :

$$ Attr_{\gamma}(x_i) = \int_{0}^{1} \frac{\partial \lambda(\gamma(t))}{\partial \gamma_i(t)} \cdot \frac{\partial \gamma_i(t)}{\partial t} dt $$

The main idea is that since the model is not linear, the gradient alone is not enough to explain the model's prediction - adressing the first issue of the methods mentioned earlier. E.g. if some very important feature happens to be on a plateau, it will not be picked up by the gradient.%TODO Evidence of this sort of thing happening is found in \citep{TODO}.
    
\citet{smilkov2017smoothgradIG, miglani2020investigatingIG, kapishnikov2021guidedGIG} improved this method of integrated gradient mainly through the choice of path and baseline which constitute the main limitations of integrated gradients, and can lead to noise in the attribution. %TOSAY : we do not use these improvements and stick to the simplest version of IG. Further work can investigate the impact of model specific vs model agnostic paths and baselines on the quality of the explanation and computational graph.

In the circuit discovery literature \citep{olah2020zoomCircuits}, people got interested in the contribution of edges or nodes in the computational graph, and used methods developped in computer vision \citep{syed2023attributionCircuits, marks2024sparseCircuits} as well as novel methods \citep{wang2022interpretabilityCircuits, conmy2023automatedACDCCircuits, ferrando2024informationCircuits, he2024dictionaryCircuits} mostly involving patching single nodes or edges to get their contribution, or linearizing the internal computations.

\subsection{Sparse Autoencoders}
\label{sec:SAE}

\paragraph{Linear Representation Hypothesis} The linear representation hypothesis assumes that a model's latent space behaves mostly linearly, with high level concepts represented as disantangled directions in this space often called features. It also states that linear directions can be found using linear probes and that they can be used as steering vectors to predictably influence the model's behavior. Evidence of that is provided by \citet{park2023linear, NIPS2013_9aa42b31, pennington2014glove, nanda2023emergent, gurnee2024language, wang2024concept, turner2024activationSteering}.

\paragraph{Sparse coding} This motivates the use of dictionary learning and sparse coding methods to find overcomplete bases able to sparsely represent the data at any point in the model. Formally, we want a familly $\{b_i \in \R^{d_{model}}\}_{i=0}^{d_{dict}}$ such that, for all activations $x$ of some module, there is a set $\{\alpha_i \in \R\}_{i=0}^{d_dict}$ mostly 0 such that $x = \sum_{i=0}^{d_dict} \alpha_i b_i$. There are two dual problems here, one is to find such a familly, the other is, given a familly and a data point, find the $\alpha_i$.

\paragraph{Notation} In the rest of this work, these $b_i$ will be unit vectors called features, and both $b_i$ and it's magnitude $\alpha_i$ will be identified under the notation $f_i$.

\paragraph{Autoencoders} People usually train sparse autoencoders (SAE) \citep{ng2011sparse} to solve both of these problems. The decoder serves as the dictionary, while the encoder is the sparse coding function.

The majority of research in transformer interpretability uses one layer MLPs \citep{cunningham2023sparse, gao2024scaling} trained to learn the identity function with an added sparsity loss. \citet{rajamanoharan2024improving} proposed an improvement to this method by adding a gated mechanism to the MLPs.

\citet{bushnaq2024using} introduced a change of basis called the \textit{Local Interaction Basis} (LIB) that can be viewed as a linear autoencoder capturing functionally important features, though the decomposition is not sparse. Any change of basis given by future research can be similarly used as a linear autoencoder.%`FUTURE`

All of this work finds higly interpretable features, and is one direction in trying to find better geometric representations of model's latent space.

\subsection{Circuit Discovery}
\label{sec:circuit_discovery}

Early work in vision models find related features across layers seeminly responsible for some common behavior \citep{olah2020zoomCircuits}. This sparked interest in the community, and recent work has been done on both vision and language transformers to find circuits of model's components working together to implement some behavior.

\citet{conmy2023automatedACDCCircuits} patches each edge with a baseline value. Edges are those of the complete computational graph, as defined by the original transformer architecture - between modules. They then compare the induced model to the unmodified one. Edges with an effect smaller than some threshold are removed. This is done for a specific task with it's associated dataset. However, this approach is very computationally expensive and doesn't scale well with model size.

\citet{syed2023attributionCircuits} reused some gradient based attribution method by comparing the gradient to some patching direction in order to have a much more efficient method.

Such methods based on whole modules are very coarse and aim at finding the role played by specific modules under a specific context. Thus, people started to try to disantangle the model's computation using more refined nodes to get a more interpretable representation.

To this aim, \citet{oneill2024sparse} used SAE's trained specifically on the task under study, with a final graph still based on heads but using information flow through these disantangled features. \citet{he2024dictionaryCircuits} also used SAE's, building dependencies between features by linearising the model's computations, removing the need for patches.% or hiding it well enough indirectly in some computation-->

\citet{marks2024sparseCircuits} also used sparse autoencoders to disantangle the model's computation. They used integrated gradients to get the contribution of each feature to the model's output, keeping only features with an attribution higher that some threshold. Their computational graph is however the complete graph between these selected features. They mention trying to attribute edges contributions too, but they do not use them in their ablation experiments.

Some work also tried to use clever changes of basis, or linear autoencoders, to disantangle the model's computation. \citet{merullo2024talking} worked purely based on the weights of the model to draw edges between singular vectors of weight matrices. \citet{bushnaq2024using} introduced a change of basis called Local Interaction Basis (LIB) that is supposed to capture functionally important features and dependencies.

All these methods seem to be able to give interesting cherry picked explanations on toy models or tasks.

\subsection{Parallels between neurosciences and mechanistic interpetability}
\label{sec:parallels}

TODO : modify this section once work has a bit more advanced, so I can say "blablabla nice parallel, and we actually tested this" instead of just "eh, random handwavy idea, please future work do it".

The recently emerging field of mechanistic interpretability (MI) seems to have been rediscovering a lot of what neuroscientists have already been doing. We believe that settling on common definitions and frameworks can help both fields as they share a lot of similarities in trying to explain the inner workings of complex systems. Having common definitions can help having a clearer global view and more sorted ideas. In particular, it can be sometimes hard to follow what fundamental distinctions exist between all mechanistic interpretability studies with lack of clear nomenclature to unambiguously refer to common concepts. It can also facilitate potential bridges and collaborations, which we believe to be numerous. In particular, MI could could use neuroscience as a foundational body of work to build upon, and neuroscience could use MI to design experiments that could validate some frameworks of network science and analysis that might be hard to validate in an actual brain.

We now give some examples of concepts and experiments that are extremely similar in both fields.

In machine learning, \textit{functional segregation} corresponds to finding some interpretable feature that single handedly explains some behavior, through probing, SAEs or any other means. It could also correspond to finding some communities of neurons in a single module that are together responsible for some behavior. \citep{TODO} showed the existence of such communities in vision classifiers, although they did not try intervening on them.

\textit{Functional integration} on the other hand would be to explain some behavior through \textit{circuit discovery}, with all three \textit{structural}, \textit{functional} and \textit{effective} connectivity having their equivalent.
\begin{itemize}
    \item[\textit{structural connectivity}] \citep{TODOclusterability} used raw weight matrices in MLPs and CNNs as \textit{structural} connections between canonical directions, also knwon as \textit{neurons}. \citep{merullo2024talking} worked based on singular decompositions of weight matrices to find \textit{structural} connections between their principal components in transformer models.
    \item[\textit{functional connectivity}] \citep{TODOvision} used biclustering on activations across examples to find \textit{functionally} similar neurons in vision models.
    \item[\textit{effective connectivity}] Most of the \textit{circuit discovery} literature builds computational graphs, or graphs of dependencies based on actual information flow across many example inputs. This seems to be the most popular framework in circuit discovery.
\end{itemize}

In both the brain and neural networks, the choice of the granularity of the nodes in the computational graph is crucial. In the brain, regions of interests can be defined by actual physical proximity which often corresond to functional proximity. In neural networks, there is no such thing. In transformer models, when people need low granularity, they usually take whole attention heads and MLP modules. This is unlikely to be be the best coarse unit of computation. We believe that introducing the term of \textit{regions of interest} in the field of MI can help people develop better ideas for grouping fine grained nodes into coarser blocks - for example by grouping together nodes identified as functionally similar in a fixed layer as in \citep{TODOvision}.

\section{Method}

\subsection{Functional connectivity}

TODO : mention the fundamental difference between functional and effective : functional *do not* imply causality and cannot define computational graphs. They can only be used to study the relationships between nodes.

corr of act/attr

used to build a graph not for getting a computational graph but a functional one, where node classification and grouping into blocks is what we care about.

used for ROI definitions

\subsection{Effective connectivity with integrated gradients}

% TODO
TODO : make the distinction between feature and region of interest, which both work exactly the same for this method. In fact, it is general (doit prod between grad & diff), and in the case of 1d features the dot product happens to simply be a product
% TODO

For all model's module $m$ - attention head, attention layer, MLP layer or residual stream (whole transformer block) - let $E_m : \R^{d_{model}} \rightarrow \R^{d_{dict}}$ and $D_m : \R^{d_{dict}} \rightarrow \R^{d_{model}}$ be an encoder and decoder pair. These can be an arbitrary autoencoder, from a linear change of basis to a gated SAE. Each of the canonical $d_{dict}$ dimensions of the dictionary will be called a feature, and the magnitude of the activation of this feature will be called the feature's activation. We chose these features as nodes for our computational graph, they are atomic functions in the grand scheme of compositions in the model. % litterally, as defined by our computational graph
We add a sink node $y$ for the model's final output. To get the dependencies between these nodes, we use the integrated gradients attribution method as follows.

Let $x \in \mathcal{D}$ be some input from a dataset $\mathcal{D}$ to the model $\mathcal{M}$. We begin by building the expanded computational graph for this input, where we duplicate each node for each token position in the input. Let $\lambda : \mathcal{M}, x \mapsto \lambda(\mathcal{M}, x)$ be a metric function on the model. For some module $m$ with output $x^m \in \R^{d_{model}}$, let $f^m := E_m(x^m) \in \R^{d_{dict}}$ be the vector of activations of this module's features. We use the integrated gradients attribution method to get the contribution of $f^m_i$ to $m(\mathcal{M}, x)$ :
$$ Attr(f^m_i) = \int_{0}^{1} \frac{\partial \lambda(\gamma(t))}{\partial \gamma_i(t)} \cdot \frac{\partial \gamma_i(t)}{\partial t} dt $$
where $\gamma : t \in [0, 1] \rightarrow t \cdot f^m \in \R^{d_{dict}}$ is the linear interpolation path between the baseline $0$ and the input $f^m$.

- To get the dependencies between $m$'s features and $y$, one can choose $\lambda$ to be a metric on the model's output, typically the value of the target logit.
- To get the dependencies between $m$'s features and some downstream module $m'$'s $i$-th feature, we typically choose $\lambda(\mathcal{M}, x) := f^{m'}_i$.

Then, let $\alpha$, $\beta$ be aggregation functions, typically among the $max$, $mean$ or $sum$ functions. We contract this expanded graph using $\alpha$, and aggregate graphs over many examples using $\beta$. For convenience of notation, we can include some thresholding or any variant thereof to get a sparser graph in these aggregation functions. Note that in practice, we typically threshold on the fly, during the construction of the expanded graph, for computational reasons.

Please also note that the choice for $\beta$ is not trivial, both $mean$ and $max$ have their drawbacks. $mean$ presents the risk to put edges coresponding to rare but important behaviors under the threshold, while keeping unimportant but consistent edges across examples. $max$ on the other hand keeps any rare behaviors, but as integrated gradient might be noisy - especially with such baseline and path - the amount of noise might be too high and the graph of dependencies might be too overcomplete. Future work can investigate the choice of better attribution functions or further pruning on the final graph.% `FUTURE`

\subsection{Validating ROIs and computational graphs}

\subsection{Evaluating a computational graph}

TODO : validation : do not use layered version of erdos graphs, it sucks. "In order to evaluate a method, one needs a random graph model for how the graph may have been generated and test the methods on these models where the ground truth is known" - a great man.

TODO : ""using correlations to define the graph based on fMRI data is garbage, one should instead fit a random graph model to the observed activation data" - another man" - the same man

TODO : apparently correlation sucks in neuro

Let $G$ be a computational graph built from a model $\mathcal{M}$. We will now evaluate the quality of this graph as well as it's interpretability. We identify $G$ a computational graph and it's induced model as a composition of functions.

\paragraph{Quality} We will evaluate the quality of the graph by comparing it to the original model. We first want to know the sufficiency of the induced model, or it's faithfulness to the original model's output, as well as it's necessity, or the unfaithfulness of it's complement. We also benchmark $G$ on typical language model tasks and compare it to the unchanged model's performance.

\paragraph{Interpretability}

- are neurons/nodes with more/less deggre/weight more/less interpretable ? What about neurons inside strong communities ?
- Interp communities & their individual nodes
    - what sequences tend to activate them ?
    - tune their sizes
    - communities of communities ? Can we see an interpretable hierarchy of complexity in the functions ?
- Let C be a community, D a dataset consisting of sequences on which this community activates.
    - kill C, measure the loss on D
    - kill V - C, measure the loss on D
    - This tells us if we can "remove" some harmful or unwanted behavior.

\section{Experiments}

ROIs and node ablation for behavioral control

Vision :

Toy CNN classifier, MNIST, other ? identify each class's "ROIs", ablate them, and see if the model still works on all other classes except this one.

Maybe AEs : pair an AE's output with a classifier and measure how we can remove it's ability to generate certain classes by ablating some ROIs.

Do that for transformers too and show that it is better than doing this with attention heads as nodes.

Language :



- SVD : empirical vs pure module weights : is it even close ? Which is more relevant ? Which's directions are more interpretable ?
    - This could be a paper of its own for some greedy people, but I think workshop paper is realistic for that... ? `FUTURE`

- run experiments specific to IOI, GT, ... tasks just to be able to compare both to other method and to ours on general tasks by benchmarking these subgraphs with all the same metrics used for general tasks.
    - validate that our methods performs at least as good as others
    - validate that these subgraphs are shit on general task and completely different from the original model, so need for a more general and still equally as interpretable representation of the model
    - what if remove task specific nodes (or edges) in the general graph ? If it greatly impact the performances, we win.

- dataset : IOI ..., wikipedia, benchmark.
    - Test on all other datasets

- graph variations and settings to test :
    - example based graph :
        - choice of baseline and path for IG `FUTURE`
        - Choice of AE : SAEs, LIB, gated SAE, sparse attribution SAE, topk SAE, SVD, Id
        - choice of modules : resid - resid vs module - module (across layers)
        - aggregation : sum vs max (both have their drawbacks)
        - thresholding vs Topk vs Top$\%$ edges ?
        - further pruning
            - remove nodes with degree (or total weight) below some threshold
            - remove nodes with degree (or total weight) above some threshold
            - starting the graph only after some layer : starting from the embedding layer might lead to too bad results : can we start from earlier layers ?
    - weight based graph :
    `FUTURE` ?
        - module vs SVD
            - "SVD" representation is very general and can be applied to any change of basis at the output of a model that one finds relevant.

- For all graph variation :
    - Faithfullness (logit, KL, accuracy, MRR, others ?) vs avg_deg vs modularity w.r.t. threshold
        - modularity with Louvain/Leiden vs modularity with correlation (either communities of nodes or of edges)
        - faithfullness/denoising/sufficiency vs completeness/noising/necessity
    - benchmark on LLMs : is it comparable to the original model ? If it is worse, is it still good enough ?
    - are neurons/nodes with more/less deggre/weight more/less interpretable ? What about neurons inside strong communities ?
    - Interp communities & their individual nodes
        - what sequences tend to activate them ?
        - tune their sizes
        - communities of communities ? Can we see an interpretable hierarchy of complexity in the functions ?
    - Let C be a community, D a dataset consisting of sequences on which this community activates.
        - kill C, measure the loss on D
        - kill V - C, measure the loss on D
        - This tells us if we can "remove" some harmful or unwanted behavior.

        
- Do it all for vision or language transformer as well as sleeper agent transformers. `FUTURE`

-> What is the "best" abstraction of a transformer model ?

- evolution of the graph across time (on model checkpoints) `FUTURE`
    - for SAEs based graphs, train them such that there is a 1-1 correspondence between the features of the two checkpoints. (same init and data, or init one as the result of the other and retrain (potentially for a small number of steps), or init one as the result of the other and freeze it, then learn a low rank transformation to add to that (Id + low rank) (either to modify the parameters or to modify the activations (second is probably more interesting but not sure)))

- evolution across size, fractional isomorphism, ... `FUTURE`

\section{limitations and future work}

\section{Conclusion}

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}