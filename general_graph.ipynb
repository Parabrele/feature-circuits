{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE : cpu\n",
      "IN_COLAB : False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the general graph of a given model\n",
    "\n",
    "Algorithm :\n",
    "Get model & dicts\n",
    "Get circuit fct\n",
    "aggregate graphs over dataset\n",
    "test this graph on dataset\n",
    "\n",
    "TODO : get circuit fct is wrong from marks\n",
    "TODO : test graph is wrong from marks (but eh, it will yield better results as we will\n",
    "       have essentially the whole graph, but hush, don't say it ! :o)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    from tqdm.notebook import tqdm, trange\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\", force_remount=True)\n",
    "    %cd /content/gdrive/MyDrive/feature-circuits\n",
    "    %pip install -r requirements.txt\n",
    "    !git submodule update --init\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    from tqdm import tqdm, trange\n",
    "\n",
    "import os\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "from dictionary_learning import AutoEncoder\n",
    "from activation_utils import SparseAct\n",
    "from buffer import TokenBuffer\n",
    "from circuit import get_circuit\n",
    "from ablation import run_with_ablations\n",
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"DEVICE :\", DEVICE)\n",
    "\n",
    "print(\"IN_COLAB :\", IN_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step : generate the graph\n",
    "\n",
    "compute the circuit for random examples from wikipedia, and aggregate the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : move tokensbuffer in a separate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia70m = LanguageModel(\n",
    "    \"EleutherAI/pythia-70m-deduped\",\n",
    "    device_map=DEVICE,\n",
    "    dispatch=True,\n",
    ")\n",
    "\n",
    "pythia70m_embed = pythia70m.gpt_neox.embed_in\n",
    "\n",
    "pythia70m_resids= []\n",
    "pythia70m_attns = []\n",
    "pythia70m_mlps = []\n",
    "for layer in range(len(pythia70m.gpt_neox.layers)):\n",
    "    pythia70m_resids.append(pythia70m.gpt_neox.layers[layer])\n",
    "    pythia70m_attns.append(pythia70m.gpt_neox.layers[layer].attention)\n",
    "    pythia70m_mlps.append(pythia70m.gpt_neox.layers[layer].mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"wikipedia\",\n",
    "    language=\"en\",\n",
    "    date=\"20240401\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    "    trust_remote_code=True\n",
    ").shuffle()\n",
    "dataset = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = TokenBuffer(\n",
    "    dataset,\n",
    "    pythia70m,\n",
    "    n_ctxs=10,\n",
    "    ctx_len=128,\n",
    "    load_buffer_batch_size=512,\n",
    "    return_batch_size=1,\n",
    "    device=DEVICE,\n",
    "    max_number_of_yields=2**20,\n",
    "    discard_bos=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    base = \"/content/gdrive/MyDrive/feature-circuits/\"\n",
    "else:\n",
    "    base = \"C:/Users/Grégoire/Documents/ENS/stages/AttentionGraph/Marks/feature-circuits/\"\n",
    "path = base + \"dictionary_learning/dictionaires/pythia-70m-deduped/\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    if IN_COLAB:\n",
    "        # go to base / dictionary_learning :\n",
    "        %cd /content/gdrive/MyDrive/feature-circuits/dictionary_learning\n",
    "        !apt-get update\n",
    "        !apt-get install dos2unix\n",
    "        !dos2unix pretrained_dictionary_downloader.sh\n",
    "        !chmod +x pretrained_dictionary_downloader.sh\n",
    "        !./pretrained_dictionary_downloader.sh\n",
    "        %cd /content/gdrive/MyDrive/feature-circuits\n",
    "    else:\n",
    "        %cd C:/Users/Grégoire/Documents/ENS/stages/AttentionGraph/Marks/feature-circuits/dictionary_learning\n",
    "        %run ./pretrained_dictionary_downloader.sh\n",
    "        %cd C:/Users/Grégoire/Documents/ENS/stages/AttentionGraph/Marks/feature-circuits\n",
    "\n",
    "dictionaries = {}\n",
    "\n",
    "d_model = 512\n",
    "dict_size = 32768\n",
    "\n",
    "ae = AutoEncoder(d_model, dict_size).to(DEVICE)\n",
    "ae.load_state_dict(torch.load(path + f\"embed/ae.pt\", map_location=DEVICE))\n",
    "dictionaries[pythia70m_embed] = ae\n",
    "\n",
    "\n",
    "for layer in range(len(pythia70m.gpt_neox.layers)):\n",
    "    ae = AutoEncoder(d_model, dict_size).to(DEVICE)\n",
    "    ae.load_state_dict(torch.load(path + f\"resid_out_layer{layer}/ae.pt\", map_location=DEVICE))\n",
    "    dictionaries[pythia70m_resids[layer]] = ae\n",
    "\n",
    "    ae = AutoEncoder(d_model, dict_size).to(DEVICE)\n",
    "    ae.load_state_dict(torch.load(path + f\"attn_out_layer{layer}/ae.pt\", map_location=DEVICE))\n",
    "    dictionaries[pythia70m_attns[layer]] = ae\n",
    "\n",
    "    ae = AutoEncoder(d_model, dict_size).to(DEVICE)\n",
    "    ae.load_state_dict(torch.load(path + f\"mlp_out_layer{layer}/ae.pt\", map_location=DEVICE))\n",
    "    dictionaries[pythia70m_mlps[layer]] = ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn_v1(model, trg=None):\n",
    "    \"\"\"\n",
    "    default : return the logit\n",
    "    \"\"\"\n",
    "    if trg is None:\n",
    "        raise ValueError(\"trg must be provided\")\n",
    "    logits = model.embed_out.output[:,-1,:]\n",
    "    return logits[torch.arange(trg.numel()), trg]\n",
    "    \n",
    "def metric_fn_v2(model, trg=None):\n",
    "    \"\"\"\n",
    "    default : return the logit\n",
    "    \"\"\"\n",
    "    if trg is None:\n",
    "        raise ValueError(\"trg must be provided\")\n",
    "    logits = model.embed_out.output[:,trg[0],:]\n",
    "    return logits[0, 0, trg[1]]\n",
    "\n",
    "def metric_fn_v3(model, trg=None):\n",
    "    \"\"\"\n",
    "    Return -log probability for the expected target.\n",
    "\n",
    "    trg : torch.Tensor, contains idxs of the target tokens (between 0 and d_vocab_out)\n",
    "\n",
    "    /!\\ here we assume that all last tokens are indeed in the last position (if padding, it must happen in front of the sequence, not after)\n",
    "    \"\"\"\n",
    "    if trg is None:\n",
    "        raise ValueError(\"trg must be provided\")\n",
    "    logits = model.embed_out.output[:,-1,:]\n",
    "    return (\n",
    "         -1 * torch.gather(\n",
    "             torch.nn.functional.log_softmax(model.embed_out.output[:,-1,:], dim=-1),\n",
    "             dim=-1, index=trg.view(-1, 1)\n",
    "         ).squeeze(-1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : if multiple GPUS, use nn.DataParallel and compute batches of length num_gpus. Each GPU will compute one input. Maybe DistributedDataParallel is better.\n",
    "\n",
    "Or : launch N instances of the code that work independently on random inputs, each on their own GPU, save the circuits in a file and then process 0 is in charge of aggregating the results. If torch provide multiprocessing communications, this can be done without storing to the disc. Then process 0 sends to all the others the final circuits, they all test it and aggregate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 128]) torch.int64\n",
      "torch.Size([1]) tensor([121])\n",
      "torch.Size([1]) tensor([783])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [10:54, 654.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([1, 128]) torch.int64\n",
      "torch.Size([1]) tensor([125])\n",
      "torch.Size([1]) tensor([187])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [22:27, 677.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([1, 128]) torch.int64\n",
      "torch.Size([1]) tensor([118])\n",
      "torch.Size([1]) tensor([41358])\n"
     ]
    }
   ],
   "source": [
    "tot_circuit = None\n",
    "i = 0\n",
    "max_loop = 10\n",
    "for tokens, trg_idx, trg in tqdm(buffer):\n",
    "    if i >= max_loop:\n",
    "        break\n",
    "    print(i)\n",
    "    print(tokens.shape, tokens.dtype)\n",
    "    print(trg_idx.shape, trg_idx)\n",
    "    print(trg.shape, trg)\n",
    "    i += 1\n",
    "    circuit = get_circuit(\n",
    "        tokens,\n",
    "        None,\n",
    "        pythia70m,\n",
    "        pythia70m_embed,\n",
    "        pythia70m_attns,\n",
    "        pythia70m_mlps,\n",
    "        pythia70m_resids,\n",
    "        dictionaries,\n",
    "        metric_fn_v2, {\"trg\": (trg_idx, trg)},\n",
    "        edge_threshold=0.1\n",
    "    )\n",
    "    if tot_circuit is None:\n",
    "        tot_circuit = circuit\n",
    "    else:\n",
    "        for k, v in circuit[0].items():\n",
    "            if v is not None:\n",
    "                tot_circuit[0][k] += v\n",
    "        for ku, vu in circuit[1].items():\n",
    "            for kd, vd in vu.items():\n",
    "                if vd is not None:\n",
    "                    tot_circuit[1][ku][kd] += vd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 50304])\n",
      "torch.Size([1, 1, 50304])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(1, 128, 50304)\n",
    "\n",
    "trg_idx = torch.tensor([126]).long()\n",
    "trg = torch.tensor([285]).long()\n",
    "print(t.shape)\n",
    "print(t[:, trg_idx, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_fn = lambda x: x.mean(dim=0).expand_as(x)\n",
    "\n",
    "# get m(C) for the circuit obtained by thresholding nodes with the given threshold\n",
    "@t.no_grad()\n",
    "def get_fcs(\n",
    "    model,\n",
    "    clean,\n",
    "    patch,\n",
    "    circuit,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    ablation_fn,\n",
    "    thresholds,\n",
    "    handle_errors = 'default', # also 'remove' or 'resid_only'\n",
    "):\n",
    "    clean_inputs = clean\n",
    "    clean_answer_idxs = trg_idx\n",
    "    patch_inputs = patch\n",
    "    patch_answer_idxs = patch_trg_idx\n",
    "\n",
    "    def metric_fn(model):\n",
    "        return (\n",
    "            - t.gather(model.embed_out.output[:,-1,:], dim=-1, index=patch_answer_idxs.view(-1, 1)).squeeze(-1) + \\\n",
    "            t.gather(model.embed_out.output[:,-1,:], dim=-1, index=clean_answer_idxs.view(-1, 1)).squeeze(-1)\n",
    "        )\n",
    "    \n",
    "    circuit = circuit[0]\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    # get F(M)\n",
    "    with model.trace(clean_inputs):\n",
    "        metric = metric_fn(model).save()\n",
    "    fm = metric.value.mean().item()\n",
    "\n",
    "    out['fm'] = fm\n",
    "\n",
    "    # get m(∅)\n",
    "    fempty = run_with_ablations(\n",
    "        clean_inputs,\n",
    "        patch_inputs,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        nodes = {\n",
    "            submod : SparseAct(\n",
    "                act=t.zeros(dict_size, dtype=t.bool), \n",
    "                resc=t.zeros(1, dtype=t.bool)).to(DEVICE)\n",
    "            for submod in submodules\n",
    "        },\n",
    "        metric_fn=metric_fn,\n",
    "        ablation_fn=ablation_fn,\n",
    "    ).mean().item()\n",
    "    out['fempty'] = fempty\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        out[threshold] = {}\n",
    "        nodes = {\n",
    "            submod : circuit[submod_names[submod]].abs() > threshold for submod in submodules\n",
    "        }\n",
    "\n",
    "        if handle_errors == 'remove':\n",
    "            for k in nodes: nodes[k].resc = t.zeros_like(nodes[k].resc, dtype=t.bool)\n",
    "        elif handle_errors == 'resid_only':\n",
    "            for k in nodes:\n",
    "                if k not in model.gpt_neox.layers: nodes[k].resc = t.zeros_like(nodes[k].resc, dtype=t.bool)\n",
    "\n",
    "        n_nodes = sum([n.act.sum() + n.resc.sum() for n in nodes.values()]).item()\n",
    "        out[threshold]['n_nodes'] = n_nodes\n",
    "        \n",
    "        out[threshold]['fc'] = run_with_ablations(\n",
    "            clean_inputs,\n",
    "            patch_inputs,\n",
    "            model,\n",
    "            submodules,\n",
    "            dictionaries,\n",
    "            nodes=nodes,\n",
    "            metric_fn=metric_fn,\n",
    "            ablation_fn=ablation_fn,\n",
    "        ).mean().item()\n",
    "        out[threshold]['fccomp'] = run_with_ablations(\n",
    "            clean_inputs,\n",
    "            patch_inputs,\n",
    "            model,\n",
    "            submodules,\n",
    "            dictionaries,\n",
    "            nodes=nodes,\n",
    "            metric_fn=metric_fn,\n",
    "            ablation_fn=ablation_fn,\n",
    "            complement=True\n",
    "        ).mean().item()\n",
    "        out[threshold]['faithfulness'] = (out[threshold]['fc'] - fempty) / (fm - fempty)\n",
    "        out[threshold]['completeness'] = (out[threshold]['fccomp'] - fempty) / (fm - fempty)\n",
    "    \n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
