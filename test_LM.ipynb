{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE : cpu\n",
      "IN_COLAB : False\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    from tqdm.notebook import tqdm, trange\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\", force_remount=True)\n",
    "    %cd /content/gdrive/MyDrive/feature-circuits\n",
    "    %pip install -r requirements.txt\n",
    "    !git submodule update --init\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    from tqdm import tqdm, trange\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "from circuit import get_circuit\n",
    "from utils import save_circuit\n",
    "from utils import plot_circuit\n",
    "from dictionary_learning import AutoEncoder\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"DEVICE :\", DEVICE)\n",
    "\n",
    "print(\"IN_COLAB :\", IN_COLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "pythia70m = LanguageModel(\"EleutherAI/pythia-70m-deduped\", device_map=DEVICE, dispatch=True)\n",
    "\n",
    "pythia70m_embed = pythia70m.gpt_neox.embed_in\n",
    "\n",
    "pythia70m_resids= []\n",
    "pythia70m_attns = []\n",
    "pythia70m_mlps = []\n",
    "for layer in range(len(pythia70m.gpt_neox.layers)):\n",
    "    pythia70m_resids.append(pythia70m.gpt_neox.layers[layer])\n",
    "    pythia70m_attns.append(pythia70m.gpt_neox.layers[layer].attention)\n",
    "    pythia70m_mlps.append(pythia70m.gpt_neox.layers[layer].mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    base = \"/content/gdrive/MyDrive/feature-circuits/\"\n",
    "else:\n",
    "    base = \"C:/Users/Grégoire/Documents/ENS/stages/AttentionGraph/Marks/feature-circuits/\"\n",
    "path = base + \"dictionary_learning/dictionaires/pythia-70m-deduped/\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    if IN_COLAB:\n",
    "        # go to base / dictionary_learning :\n",
    "        %cd /content/gdrive/MyDrive/feature-circuits/dictionary_learning\n",
    "        !apt-get update\n",
    "        !apt-get install dos2unix\n",
    "        !dos2unix pretrained_dictionary_downloader.sh\n",
    "        !chmod +x pretrained_dictionary_downloader.sh\n",
    "        !./pretrained_dictionary_downloader.sh\n",
    "        %cd /content/gdrive/MyDrive/feature-circuits\n",
    "    else:\n",
    "        %cd C:/Users/Grégoire/Documents/ENS/stages/AttentionGraph/Marks/feature-circuits/dictionary_learning\n",
    "        %run ./pretrained_dictionary_downloader.sh\n",
    "        %cd C:/Users/Grégoire/Documents/ENS/stages/AttentionGraph/Marks/feature-circuits\n",
    "\n",
    "dictionaries = {}\n",
    "\n",
    "d_model = 512\n",
    "dict_size = 32768\n",
    "\n",
    "ae = AutoEncoder(d_model, dict_size).to(DEVICE)\n",
    "ae.load_state_dict(torch.load(path + f\"embed/ae.pt\", map_location=DEVICE))\n",
    "dictionaries[pythia70m_embed] = ae\n",
    "\n",
    "\n",
    "for layer in range(len(pythia70m.gpt_neox.layers)):\n",
    "    ae = AutoEncoder(d_model, dict_size).to(DEVICE)\n",
    "    ae.load_state_dict(torch.load(path + f\"resid_out_layer{layer}/ae.pt\", map_location=DEVICE))\n",
    "    dictionaries[pythia70m_resids[layer]] = ae\n",
    "\n",
    "    # ae = AutoEncoder(d_model, dict_size).to(DEVICE)\n",
    "    # ae.load_state_dict(torch.load(path + f\"attn_out_layer{layer}/ae.pt\", map_location=DEVICE))\n",
    "    # dictionaries[pythia70m_attns[layer]] = ae\n",
    "\n",
    "    # ae = AutoEncoder(d_model, dict_size).to(DEVICE)\n",
    "    # ae.load_state_dict(torch.load(path + f\"mlp_out_layer{layer}/ae.pt\", map_location=DEVICE))\n",
    "    # dictionaries[pythia70m_mlps[layer]] = ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn_v1(model, trg=None):\n",
    "    \"\"\"\n",
    "    default : return the logit\n",
    "    \"\"\"\n",
    "    if trg is None:\n",
    "        raise ValueError(\"trg must be provided\")\n",
    "    logits = model.embed_out.output[:,-1,:]\n",
    "    return logits[torch.arange(trg.numel()), trg]\n",
    "    \n",
    "def metric_fn_v2(model, trg=None):\n",
    "    \"\"\"\n",
    "    default : return the logit\n",
    "    \"\"\"\n",
    "    if trg is None:\n",
    "        raise ValueError(\"trg must be provided\")\n",
    "    logits = model.embed_out.output[:,trg[0],:]\n",
    "    return logits[0, 0, trg[1]]\n",
    "\n",
    "def metric_fn_v3(model, trg=None):\n",
    "    \"\"\"\n",
    "    Return -log probability for the expected target.\n",
    "\n",
    "    trg : torch.Tensor, contains idxs of the target tokens (between 0 and d_vocab_out)\n",
    "\n",
    "    /!\\ here we assume that all last tokens are indeed in the last position (if padding, it must happen in front of the sequence, not after)\n",
    "    \"\"\"\n",
    "    if trg is None:\n",
    "        raise ValueError(\"trg must be provided\")\n",
    "    logits = model.embed_out.output[:,-1,:]\n",
    "    return (\n",
    "         -1 * torch.gather(\n",
    "             torch.nn.functional.log_softmax(model.embed_out.output[:,-1,:], dim=-1),\n",
    "             dim=-1, index=trg.view(-1, 1)\n",
    "         ).squeeze(-1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6393])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "clean = [\n",
    "    \"When Mary and John went to the store, John gave a drink to\"\n",
    "    for _ in range(batch_size)\n",
    "]\n",
    "patch = None\n",
    "\n",
    "trg = \" Mary\"\n",
    "trg_idx = torch.tensor([pythia70m.tokenizer.encode(trg)[0]] * batch_size, device=DEVICE)\n",
    "print(trg_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing effects for layer 5 with 1 features\n",
      "Computing effects for layer 4 with 1 features\n",
      "Computing effects for layer 3 with 1 features\n",
      "Computing effects for layer 2 with 12 features\n",
      "Computing effects for layer 1 with 6 features\n",
      "Computing effects for layer 0 with 1 features\n"
     ]
    }
   ],
   "source": [
    "circuit = get_circuit(\n",
    "    clean, patch,\n",
    "    pythia70m,\n",
    "    dictionaries,\n",
    "    metric_fn_v1,\n",
    "    pythia70m_embed, pythia70m_resids,\n",
    "    metric_kwargs={\"trg\": trg_idx},\n",
    "    original_marks=False,\n",
    "    edge_threshold=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now looking at edges from upstream embed\n",
      "arriving at downstream resid_0\n",
      "Now looking at edges from upstream resid_0\n",
      "arriving at downstream resid_1\n",
      "Now looking at edges from upstream resid_1\n",
      "arriving at downstream resid_2\n",
      "Now looking at edges from upstream resid_2\n",
      "arriving at downstream resid_3\n",
      "Now looking at edges from upstream resid_3\n",
      "arriving at downstream resid_4\n",
      "Now looking at edges from upstream resid_4\n",
      "arriving at downstream resid_5\n",
      "Now looking at edges from upstream resid_5\n",
      "arriving at downstream y\n"
     ]
    }
   ],
   "source": [
    "import evaluation\n",
    "import importlib\n",
    "importlib.reload(evaluation)\n",
    "\n",
    "mask = evaluation.get_mask(circuit, 0.5)\n",
    "pruned = evaluation.prune(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 267])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.rand((512,)) > 0.5\n",
    "idxs = torch.randint(0, 512, (2, 512))\n",
    "new_idxs = idxs[:, mask]\n",
    "print(new_idxs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cpu :\n",
    "    - 1 : 2m47\n",
    "    - 2 : /\n",
    "    - 10: Stop at 68m+\n",
    "\n",
    "- gpu :\n",
    "    - 1 : 42s\n",
    "    - 2 : 1m32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submod_1 = \"resid_0\"\n",
    "submod_2 = \"resid_1\"\n",
    "\n",
    "weights = circuit[1][submod_1][submod_2]\n",
    "weights = weights.values()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "alive_downstream = circuit[1][submod_1][submod_2].indices()[0]\n",
    "set_downstream = list(set([alive_downstream_.item() for alive_downstream_ in alive_downstream]))\n",
    "\n",
    "ss = []\n",
    "abss = []\n",
    "nb_k = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for k in tqdm(set_downstream):\n",
    "    weights = []\n",
    "    for i, idx in enumerate(alive_downstream):\n",
    "        if idx == k:\n",
    "            weights.append(circuit[1][submod_1][submod_2].values()[i])\n",
    "    weights = torch.stack(weights)\n",
    "\n",
    "    perm = torch.argsort(weights.abs(), descending=True)\n",
    "    weights = weights[perm]\n",
    "    tot = sum(weights)\n",
    "    s = 0\n",
    "    for i in range(len(weights)):\n",
    "        s += weights[i]\n",
    "        if i < len(ss):\n",
    "            ss[i] += (s / tot).item()\n",
    "        else:\n",
    "            ss.append((s / tot).item())\n",
    "        if i < len(abss):\n",
    "            abss[i] += weights[i].abs().item()\n",
    "        else:\n",
    "            abss.append(weights[i].abs().item())\n",
    "        if i < len(nb_k):\n",
    "            nb_k[i] += 1\n",
    "        else:\n",
    "            nb_k.append(1)\n",
    "        # print(\"i :\", i)\n",
    "        # print(\"weight :\", embed_weights[i].item())\n",
    "        # print(\"% of total :\", s.item() / tot.item() * 100)\n",
    "\n",
    "ss = [ss[i] / nb_k[i] for i in range(len(ss))]\n",
    "abss = [abss[i] / nb_k[i] for i in range(len(abss))]\n",
    "\n",
    "\"\"\"\n",
    "plot ss and abss on two different axis with the same x-axis on the same plot\n",
    "\"\"\"\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('weight index')\n",
    "ax1.set_ylabel('cumulative % of total', color=color)\n",
    "ax1.plot(ss, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('weight', color=color)\n",
    "ax2.plot(abss, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_weights = 100\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('weight index')\n",
    "ax1.set_ylabel('cumulative % of total', color=color)\n",
    "ax1.plot(ss[:max_weights], color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('weight', color=color)\n",
    "ax2.plot(abss[:max_weights], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import circuit_plotting\n",
    "importlib.reload(circuit_plotting)\n",
    "circuit_plotting.plot_circuit(circuit[0], circuit[1], save_dir='./circuit/cpu_2_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = []\n",
    "for key, value in circuit[1].items():\n",
    "    for k, v in value.items():\n",
    "        all_weights.append(v.values())\n",
    "        \n",
    "all_weights = torch.cat(all_weights, dim=0)\n",
    "print(all_weights.shape)\n",
    "print(all_weights.abs().mean())\n",
    "\n",
    "plt.hist(all_weights[all_weights.abs() > 0.01].detach().cpu().numpy(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(1, 10, 50)\n",
    "B = torch.randn(1, 10, 50)\n",
    "\n",
    "print((A * B).shape)\n",
    "print(A @ B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dummy_2d_sparse_idx = torch.tensor([[0, 99, 27], [1, 2, 199]])\n",
    "dummy_2d_sparse_values = torch.randn(2, 3)\n",
    "\n",
    "dummy_2d_sparse = torch.sparse_coo_tensor(\n",
    "    dummy_2d_sparse_idx,\n",
    "    dummy_2d_sparse_values,\n",
    "    size=(100, 200)\n",
    ")\n",
    "\n",
    "print(dummy_2d_sparse.to_dense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hehehe_hahaha\n",
      "['hehehe', 'hahaha']\n",
      "['hehehehahaha']\n"
     ]
    }
   ],
   "source": [
    "print(\"hehehe_hahaha\")\n",
    "print(\"hehehe_hahaha\".split(\"_\"))\n",
    "print(\"hehehehahaha\".split(\"_\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
