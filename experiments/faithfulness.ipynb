{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ConnardMcGregoire\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath('..')\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from activation_utils import SparseAct\n",
    "import torch as t\n",
    "import plotly.graph_objects as go\n",
    "from loading_utils import load_examples\n",
    "from dictionary_learning import AutoEncoder\n",
    "from dictionary_learning.dictionary import IdentityDict\n",
    "from ablation import run_with_ablations\n",
    "from scipy import interpolate\n",
    "import math\n",
    "from statistics import stdev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ConnardMcGregoire\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map=device, dispatch=True)\n",
    "\n",
    "start_layer = 2 # explain the model starting here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load submodules\n",
    "submodules = []\n",
    "if start_layer < 0: submodules.append(model.gpt_neox.embed_in)\n",
    "for i in range(start_layer, len(model.gpt_neox.layers)):\n",
    "    submodules.extend([\n",
    "        model.gpt_neox.layers[i].attention,\n",
    "        model.gpt_neox.layers[i].mlp,\n",
    "        model.gpt_neox.layers[i]\n",
    "    ])\n",
    "\n",
    "submod_names = {\n",
    "    model.gpt_neox.embed_in : 'embed'\n",
    "}\n",
    "for i in range(len(model.gpt_neox.layers)):\n",
    "    submod_names[model.gpt_neox.layers[i].attention] = f'attn_{i}'\n",
    "    submod_names[model.gpt_neox.layers[i].mlp] = f'mlp_{i}'\n",
    "    submod_names[model.gpt_neox.layers[i]] = f'resid_{i}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionaries\n",
    "dict_id = 10\n",
    "\n",
    "activation_dim = 512\n",
    "expansion_factor = 64\n",
    "dict_size = expansion_factor * activation_dim\n",
    "\n",
    "feat_dicts = {}\n",
    "ae = AutoEncoder(activation_dim, dict_size).to(device)\n",
    "ae.load_state_dict(t.load(f\"C:/Users/ConnardMcGregoire/Documents/MI_Internship/feature-circuits/dictionary_learning/dictionaires/pythia-70m-deduped/embed/ae.pt\", map_location=device))\n",
    "feat_dicts[model.gpt_neox.embed_in] = ae\n",
    "\n",
    "d_model = activation_dim\n",
    "dict_size = d_model * expansion_factor\n",
    "\n",
    "for layer in range(len(model.gpt_neox.layers)):\n",
    "    ae = AutoEncoder(d_model, dict_size).to(device)\n",
    "    ae.load_state_dict(t.load(f\"C:/Users/ConnardMcGregoire/Documents/MI_Internship/feature-circuits/dictionary_learning/dictionaires/pythia-70m-deduped/resid_out_layer{layer}/ae.pt\", map_location=device))\n",
    "    feat_dicts[model.gpt_neox.layers[i]] = ae\n",
    "\n",
    "    ae = AutoEncoder(d_model, dict_size).to(device)\n",
    "    ae.load_state_dict(t.load(f\"C:/Users/ConnardMcGregoire/Documents/MI_Internship/feature-circuits/dictionary_learning/dictionaires/pythia-70m-deduped/attn_out_layer{layer}/ae.pt\", map_location=device))\n",
    "    feat_dicts[model.gpt_neox.layers[i].attention] = ae\n",
    "\n",
    "    ae = AutoEncoder(d_model, dict_size).to(device)\n",
    "    ae.load_state_dict(t.load(f\"C:/Users/ConnardMcGregoire/Documents/MI_Internship/feature-circuits/dictionary_learning/dictionaires/pythia-70m-deduped/mlp_out_layer{layer}/ae.pt\", map_location=device))\n",
    "    feat_dicts[model.gpt_neox.layers[i].mlp] = ae\n",
    "\n",
    "neuron_dicts = {\n",
    "    submod : IdentityDict(activation_dim).to(device) for submod in submodules\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = \"When Mary and John went to the store, John gave a drink to\"\n",
    "patch = \"When Mary and John went to the store, Alice gave a drink to\"\n",
    "\n",
    "circuit = get_circuit(\n",
    "    clean, patch,\n",
    "    pythia70m,\n",
    "    pythia70m_embed, pythia70m_attns, pythia70m_mlps, pythia70m_resids, dictionaries,\n",
    "    metric_fn_v1, {\"trg\": trg_idx}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mean ablation\n",
    "ablation_fn = lambda x: x.mean(dim=0).expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get m(C) for the circuit obtained by thresholding nodes with the given threshold\n",
    "def get_fcs(\n",
    "        dataset,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        ablation_fn,\n",
    "        thresholds,\n",
    "        length,\n",
    "        handle_errors = 'default', # also 'remove' or 'resid_only'\n",
    "        use_neurons = False,\n",
    "        random = False\n",
    "):\n",
    "    # load data \n",
    "    if not use_neurons:\n",
    "        circuit = t.load(f'../circuits/{dataset}_train_dict10_node0.1_edge0.01_n100_aggnone.pt')['nodes']\n",
    "    else:\n",
    "        circuit = t.load(f'../circuits/{dataset}_train_dictid_node0.1_edge0.01_n100_aggnone.pt')['nodes']\n",
    "    examples = load_examples(f'/share/projects/dictionary_circuits/data/phenomena/{dataset}_test.json', 40, model, length=length)\n",
    "    clean_inputs = t.cat([e['clean_prefix'] for e in examples], dim=0).to('cuda:0')\n",
    "    clean_answer_idxs = t.tensor([e['clean_answer'] for e in examples], dtype=t.long, device='cuda:0')\n",
    "    patch_inputs = t.cat([e['patch_prefix'] for e in examples], dim=0).to('cuda:0')\n",
    "    patch_answer_idxs = t.tensor([e['patch_answer'] for e in examples], dtype=t.long, device='cuda:0')\n",
    "    def metric_fn(model):\n",
    "        return (\n",
    "            - t.gather(model.embed_out.output[:,-1,:], dim=-1, index=patch_answer_idxs.view(-1, 1)).squeeze(-1) + \\\n",
    "            t.gather(model.embed_out.output[:,-1,:], dim=-1, index=clean_answer_idxs.view(-1, 1)).squeeze(-1)\n",
    "        )\n",
    "    \n",
    "    with t.no_grad():\n",
    "        out = {}\n",
    "\n",
    "        # get F(M)\n",
    "        with model.trace(clean_inputs):\n",
    "            metric = metric_fn(model).save()\n",
    "        fm = metric.value.mean().item()\n",
    "\n",
    "        out['fm'] = fm\n",
    "\n",
    "        # get m(âˆ…)\n",
    "        fempty = run_with_ablations(\n",
    "            clean_inputs,\n",
    "            patch_inputs,\n",
    "            model,\n",
    "            submodules,\n",
    "            dictionaries,\n",
    "            nodes = {\n",
    "                submod : SparseAct(\n",
    "                    act=t.zeros(dict_size if not use_neurons else activation_dim, dtype=t.bool), \n",
    "                    resc=t.zeros(1, dtype=t.bool)).to(device)\n",
    "                for submod in submodules\n",
    "            },\n",
    "            metric_fn=metric_fn,\n",
    "            ablation_fn=ablation_fn,\n",
    "        ).mean().item()\n",
    "        out['fempty'] = fempty\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            out[threshold] = {}\n",
    "            nodes = {\n",
    "                submod : circuit[submod_names[submod]].abs() > threshold for submod in submodules\n",
    "            }\n",
    "\n",
    "            if handle_errors == 'remove':\n",
    "                for k in nodes: nodes[k].resc = t.zeros_like(nodes[k].resc, dtype=t.bool)\n",
    "            elif handle_errors == 'resid_only':\n",
    "                for k in nodes:\n",
    "                    if k not in model.gpt_neox.layers: nodes[k].resc = t.zeros_like(nodes[k].resc, dtype=t.bool)\n",
    "\n",
    "            n_nodes = sum([n.act.sum() + n.resc.sum() for n in nodes.values()]).item()\n",
    "            if random:\n",
    "                total_nodes = sum([n.act.numel() + n.resc.numel() for n in nodes.values()])\n",
    "                p = n_nodes / total_nodes\n",
    "                for k in nodes:\n",
    "                    nodes[k].act = t.bernoulli(t.ones_like(nodes[k].act, dtype=t.float) * p).to(device).to(dtype=t.bool)\n",
    "                    nodes[k].resc = t.ones_like(nodes[k].resc, dtype=t.bool).to(device)\n",
    "                out[threshold]['n_nodes'] = sum([n.act.sum() + n.resc.sum() for n in nodes.values()]).item()\n",
    "            else:\n",
    "                out[threshold]['n_nodes'] = n_nodes\n",
    "            \n",
    "            out[threshold]['fc'] = run_with_ablations(\n",
    "                clean_inputs,\n",
    "                patch_inputs,\n",
    "                model,\n",
    "                submodules,\n",
    "                dictionaries,\n",
    "                nodes=nodes,\n",
    "                metric_fn=metric_fn,\n",
    "                ablation_fn=ablation_fn,\n",
    "            ).mean().item()\n",
    "            out[threshold]['fccomp'] = run_with_ablations(\n",
    "                clean_inputs,\n",
    "                patch_inputs,\n",
    "                model,\n",
    "                submodules,\n",
    "                dictionaries,\n",
    "                nodes=nodes,\n",
    "                metric_fn=metric_fn,\n",
    "                ablation_fn=ablation_fn,\n",
    "                complement=True\n",
    "            ).mean().item()\n",
    "            out[threshold]['faithfulness'] = (out[threshold]['fc'] - fempty) / (fm - fempty)\n",
    "            out[threshold]['completeness'] = (out[threshold]['fccomp'] - fempty) / (fm - fempty)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../circuits/rc_train_dict10_node0.1_edge0.01_n100_aggnone.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m\n\u001b[0;32m      2\u001b[0m datasets \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrc\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m6\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnounpp\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimple\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwithin_rc\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      7\u001b[0m }\n\u001b[0;32m      8\u001b[0m thresholds \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mlogspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      9\u001b[0m outs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[43m{\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_fcs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubmodules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeat_dicts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mablation_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mablation_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m,\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures_wo_errs\u001b[39m\u001b[38;5;124m'\u001b[39m : {\n\u001b[0;32m     22\u001b[0m         dataset : get_fcs(\n\u001b[0;32m     23\u001b[0m             dataset,\n\u001b[0;32m     24\u001b[0m             model,\n\u001b[0;32m     25\u001b[0m             submodules,\n\u001b[0;32m     26\u001b[0m             feat_dicts,\n\u001b[0;32m     27\u001b[0m             ablation_fn\u001b[38;5;241m=\u001b[39mablation_fn,\n\u001b[0;32m     28\u001b[0m             thresholds \u001b[38;5;241m=\u001b[39m thresholds,\n\u001b[0;32m     29\u001b[0m             length\u001b[38;5;241m=\u001b[39mlength,\n\u001b[0;32m     30\u001b[0m             handle_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     31\u001b[0m         ) \u001b[38;5;28;01mfor\u001b[39;00m dataset, length \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     32\u001b[0m     },\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures_wo_some_errs\u001b[39m\u001b[38;5;124m'\u001b[39m : {\n\u001b[0;32m     34\u001b[0m         dataset : get_fcs(\n\u001b[0;32m     35\u001b[0m             dataset,\n\u001b[0;32m     36\u001b[0m             model,\n\u001b[0;32m     37\u001b[0m             submodules,\n\u001b[0;32m     38\u001b[0m             feat_dicts,\n\u001b[0;32m     39\u001b[0m             ablation_fn\u001b[38;5;241m=\u001b[39mablation_fn,\n\u001b[0;32m     40\u001b[0m             thresholds \u001b[38;5;241m=\u001b[39m thresholds,\n\u001b[0;32m     41\u001b[0m             length\u001b[38;5;241m=\u001b[39mlength,\n\u001b[0;32m     42\u001b[0m             handle_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresid_only\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     43\u001b[0m         ) \u001b[38;5;28;01mfor\u001b[39;00m dataset, length \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     44\u001b[0m     },\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneurons\u001b[39m\u001b[38;5;124m'\u001b[39m : {\n\u001b[0;32m     46\u001b[0m         dataset : get_fcs(\n\u001b[0;32m     47\u001b[0m             dataset,\n\u001b[0;32m     48\u001b[0m             model,\n\u001b[0;32m     49\u001b[0m             submodules,\n\u001b[0;32m     50\u001b[0m             neuron_dicts,\n\u001b[0;32m     51\u001b[0m             ablation_fn\u001b[38;5;241m=\u001b[39mablation_fn,\n\u001b[0;32m     52\u001b[0m             thresholds \u001b[38;5;241m=\u001b[39m thresholds,\n\u001b[0;32m     53\u001b[0m             length\u001b[38;5;241m=\u001b[39mlength,\n\u001b[0;32m     54\u001b[0m             use_neurons\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         ) \u001b[38;5;28;01mfor\u001b[39;00m dataset, length \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     56\u001b[0m     },\n\u001b[0;32m     57\u001b[0m }\n",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m datasets \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrc\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m6\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnounpp\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimple\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwithin_rc\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      7\u001b[0m }\n\u001b[0;32m      8\u001b[0m thresholds \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mlogspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      9\u001b[0m outs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m : {\n\u001b[1;32m---> 11\u001b[0m         dataset : \u001b[43mget_fcs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubmodules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeat_dicts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mablation_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mablation_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m dataset, length \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     20\u001b[0m     },\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures_wo_errs\u001b[39m\u001b[38;5;124m'\u001b[39m : {\n\u001b[0;32m     22\u001b[0m         dataset : get_fcs(\n\u001b[0;32m     23\u001b[0m             dataset,\n\u001b[0;32m     24\u001b[0m             model,\n\u001b[0;32m     25\u001b[0m             submodules,\n\u001b[0;32m     26\u001b[0m             feat_dicts,\n\u001b[0;32m     27\u001b[0m             ablation_fn\u001b[38;5;241m=\u001b[39mablation_fn,\n\u001b[0;32m     28\u001b[0m             thresholds \u001b[38;5;241m=\u001b[39m thresholds,\n\u001b[0;32m     29\u001b[0m             length\u001b[38;5;241m=\u001b[39mlength,\n\u001b[0;32m     30\u001b[0m             handle_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     31\u001b[0m         ) \u001b[38;5;28;01mfor\u001b[39;00m dataset, length \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     32\u001b[0m     },\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures_wo_some_errs\u001b[39m\u001b[38;5;124m'\u001b[39m : {\n\u001b[0;32m     34\u001b[0m         dataset : get_fcs(\n\u001b[0;32m     35\u001b[0m             dataset,\n\u001b[0;32m     36\u001b[0m             model,\n\u001b[0;32m     37\u001b[0m             submodules,\n\u001b[0;32m     38\u001b[0m             feat_dicts,\n\u001b[0;32m     39\u001b[0m             ablation_fn\u001b[38;5;241m=\u001b[39mablation_fn,\n\u001b[0;32m     40\u001b[0m             thresholds \u001b[38;5;241m=\u001b[39m thresholds,\n\u001b[0;32m     41\u001b[0m             length\u001b[38;5;241m=\u001b[39mlength,\n\u001b[0;32m     42\u001b[0m             handle_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresid_only\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     43\u001b[0m         ) \u001b[38;5;28;01mfor\u001b[39;00m dataset, length \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     44\u001b[0m     },\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneurons\u001b[39m\u001b[38;5;124m'\u001b[39m : {\n\u001b[0;32m     46\u001b[0m         dataset : get_fcs(\n\u001b[0;32m     47\u001b[0m             dataset,\n\u001b[0;32m     48\u001b[0m             model,\n\u001b[0;32m     49\u001b[0m             submodules,\n\u001b[0;32m     50\u001b[0m             neuron_dicts,\n\u001b[0;32m     51\u001b[0m             ablation_fn\u001b[38;5;241m=\u001b[39mablation_fn,\n\u001b[0;32m     52\u001b[0m             thresholds \u001b[38;5;241m=\u001b[39m thresholds,\n\u001b[0;32m     53\u001b[0m             length\u001b[38;5;241m=\u001b[39mlength,\n\u001b[0;32m     54\u001b[0m             use_neurons\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         ) \u001b[38;5;28;01mfor\u001b[39;00m dataset, length \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     56\u001b[0m     },\n\u001b[0;32m     57\u001b[0m }\n",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m, in \u001b[0;36mget_fcs\u001b[1;34m(dataset, model, submodules, dictionaries, ablation_fn, thresholds, length, handle_errors, use_neurons, random)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_fcs\u001b[39m(\n\u001b[0;32m      3\u001b[0m         dataset,\n\u001b[0;32m      4\u001b[0m         model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m ):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# load data \u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_neurons:\n\u001b[1;32m---> 16\u001b[0m         circuit \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../circuits/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_train_dict10_node0.1_edge0.01_n100_aggnone.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnodes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m         circuit \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../circuits/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_train_dictid_node0.1_edge0.01_n100_aggnone.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnodes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ConnardMcGregoire\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\ConnardMcGregoire\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\ConnardMcGregoire\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../circuits/rc_train_dict10_node0.1_edge0.01_n100_aggnone.pt'"
     ]
    }
   ],
   "source": [
    "# dataset : number of tokens in inputs from dataset\n",
    "datasets = {\n",
    "    'rc' : 6,\n",
    "    'nounpp' : 5,\n",
    "    'simple' : 2,\n",
    "    'within_rc' : 5\n",
    "}\n",
    "thresholds = t.logspace(-4, 0, 15, 10).tolist()\n",
    "outs = {\n",
    "    'features' : {\n",
    "        dataset : get_fcs(\n",
    "            dataset,\n",
    "            model,\n",
    "            submodules,\n",
    "            feat_dicts,\n",
    "            ablation_fn=ablation_fn,\n",
    "            thresholds = thresholds,\n",
    "            length=length,\n",
    "        ) for dataset, length in datasets.items()\n",
    "    },\n",
    "    'features_wo_errs' : {\n",
    "        dataset : get_fcs(\n",
    "            dataset,\n",
    "            model,\n",
    "            submodules,\n",
    "            feat_dicts,\n",
    "            ablation_fn=ablation_fn,\n",
    "            thresholds = thresholds,\n",
    "            length=length,\n",
    "            handle_errors='remove'\n",
    "        ) for dataset, length in datasets.items()\n",
    "    },\n",
    "    'features_wo_some_errs' : {\n",
    "        dataset : get_fcs(\n",
    "            dataset,\n",
    "            model,\n",
    "            submodules,\n",
    "            feat_dicts,\n",
    "            ablation_fn=ablation_fn,\n",
    "            thresholds = thresholds,\n",
    "            length=length,\n",
    "            handle_errors='resid_only'\n",
    "        ) for dataset, length in datasets.items()\n",
    "    },\n",
    "    'neurons' : {\n",
    "        dataset : get_fcs(\n",
    "            dataset,\n",
    "            model,\n",
    "            submodules,\n",
    "            neuron_dicts,\n",
    "            ablation_fn=ablation_fn,\n",
    "            thresholds = thresholds,\n",
    "            length=length,\n",
    "            use_neurons=True\n",
    "        ) for dataset, length in datasets.items()\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot faithfulness results\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = {\n",
    "    'features' : 'blue',\n",
    "    'features_wo_errs' : 'red',\n",
    "    'features_wo_some_errs' : 'green',\n",
    "    'neurons' : 'purple',\n",
    "    # 'random_features' : 'black'\n",
    "}\n",
    "\n",
    "for setting, subouts in outs.items():\n",
    "\n",
    "    x_min = max([min(subouts[dataset][t]['n_nodes'] for t in thresholds) for dataset in datasets]) + 1\n",
    "    x_max = min([max(subouts[dataset][t]['n_nodes'] for t in thresholds) for dataset in datasets]) - 1\n",
    "    fs = {\n",
    "        dataset : interpolate.interp1d([subouts[dataset][t]['n_nodes'] for t in thresholds], [subouts[dataset][t]['faithfulness'] for t in thresholds])\n",
    "        for dataset in datasets\n",
    "    }\n",
    "    xs = t.logspace(math.log10(x_min), math.log10(x_max), 100, 10).tolist()\n",
    "\n",
    "    for dataset in datasets:\n",
    "\n",
    "        \n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x = [subouts[dataset][t]['n_nodes'] for t in thresholds],\n",
    "            y = [subouts[dataset][t]['faithfulness'] for t in thresholds],\n",
    "            mode='lines', line=dict(color=colors[setting]), opacity=0.17, showlegend=False\n",
    "        ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=xs,\n",
    "        y=[ sum([f(x) for f in fs.values()]) / len(fs) for x in xs ],\n",
    "        mode='lines', line=dict(color=colors[setting]), name=setting\n",
    "    ))\n",
    "\n",
    "fig.update_xaxes(range=(0, 1700))\n",
    "fig.update_yaxes(range=(0, 1.1))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Nodes',\n",
    "    yaxis_title='Faithfulness',\n",
    "    width=800,\n",
    "    height=375,\n",
    "    # set white background color\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "    # add grey gridlines\n",
    "    yaxis=dict(gridcolor='rgb(200,200,200)',mirror=True,ticks='outside',showline=True),\n",
    "    xaxis=dict(gridcolor='rgb(200,200,200)', mirror=True, ticks='outside', showline=True),\n",
    "\n",
    ")\n",
    "\n",
    "# fig.show()\n",
    "fig.write_image('faithfulness.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot completeness results\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = {\n",
    "    'features' : 'blue',\n",
    "    'features_wo_errs' : 'red',\n",
    "    'features_wo_some_errs' : 'green',\n",
    "    'neurons' : 'purple'\n",
    "}\n",
    "\n",
    "for setting, subouts in outs.items():\n",
    "\n",
    "    x_min = max([min(subouts[dataset][t]['n_nodes'] for t in thresholds) for dataset in datasets]) + 1\n",
    "    x_max = min([max(subouts[dataset][t]['n_nodes'] for t in thresholds) for dataset in datasets]) - 1\n",
    "    fs = {\n",
    "        dataset : interpolate.interp1d([subouts[dataset][t]['n_nodes'] for t in thresholds], [subouts[dataset][t]['completeness'] for t in thresholds])\n",
    "        for dataset in datasets\n",
    "    }\n",
    "    xs = t.logspace(math.log10(x_min), math.log10(x_max), 100, 10).tolist()\n",
    "\n",
    "    for dataset in datasets:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x = [subouts[dataset][t]['n_nodes'] for t in thresholds],\n",
    "            y = [subouts[dataset][t]['completeness'] for t in thresholds],\n",
    "            mode='lines', line=dict(color=colors[setting]), opacity=0.17, showlegend=False\n",
    "        ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=xs,\n",
    "        y=[ sum([f(x) for f in fs.values()]) / len(fs) for x in xs ],\n",
    "        mode='lines', line=dict(color=colors[setting]), name=setting\n",
    "    ))\n",
    "\n",
    "fig.update_xaxes(range=(0,300))\n",
    "fig.update_yaxes(range=(-.15, 1))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Nodes',\n",
    "    yaxis_title='Faithfulness',\n",
    "    width=800,\n",
    "    height=375,\n",
    "    # set white background color\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "    # add grey gridlines\n",
    "    yaxis=dict(gridcolor='rgb(200,200,200)',mirror=True,ticks='outside',showline=True),\n",
    "    xaxis=dict(gridcolor='rgb(200,200,200)', mirror=True, ticks='outside', showline=True),\n",
    ")\n",
    "# fig.show()\n",
    "fig.write_image('completeness.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
